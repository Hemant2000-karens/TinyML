{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX4Kg8DUTKWO"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "Before starting, you must click on the \"Copy To Drive\" option in the top bar. Go to File --> Save a Copy to Drive. Name it *'LastName_FirstName_T1.5.ipynb'*. <ins>This is the master notebook so you will not be able to save your changes without copying it !</ins> Once you click on that, make sure you are working on that version of the notebook so that your work is saved.\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ucDy60bIimay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### In this Colab we'll explore a more advanced Image Dataset - Horses vs. Humans. We’ll start by downloading and organizing the image database using Image Generators. We’ll then train a model with and without Image Augmentation and then finally evaluate our model by uploading a horse or human image and see if the model can classify it correctly. In the end, we’ll visualize the model layers."
      ],
      "metadata": {
        "id": "IyTgdj4cqmSK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Binary Classification\n",
        "\n",
        "We are trying to classify between two different classes - horses or humans - therefore, making this a Binary Classification task!\n",
        "\n",
        "Unlike in the previous tutorials where we had imported all necessary libraries in the beginning, here we will import them when required (in the required cell). This way we'd see the immediate use of the imported library."
      ],
      "metadata": {
        "id": "PE6zRLn-R66G"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioLbtB3uGKPX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f2f2f00-5220-4051-8029-fa1ea8f30ccc"
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import:\n",
        "1. [tensorflow](https://www.tensorflow.org/) - open source library to develop and train ML models"
      ],
      "metadata": {
        "id": "4YNFeDzUpSnU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvfZg3LQbD-5"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyiYQL32R3Vg"
      },
      "source": [
        "### Download the Dataset into our Colab Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "We download the zip files for the training dataset (*train-horse-or-human.zip*) and validation dataset (*validation-horse-or-human.zip*). The links to these datasets - [train_dataset](https://drive.google.com/uc?id=1hJVjhXUNTs0GQP0MHH__sliYSkfEHgXE) and [validation_dataset](https://drive.google.com/uc?id=1tjXOTk_Ha1J7YqANFLYrw1ldb1IuOPr_).\n",
        "\n",
        "You can find these two .zip files on the left i.e., in your content/ folder after running the below cell.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wsiDQlKnmuVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --no-cache-dir gdown\n",
        "\n",
        "# download train set\n",
        "!gdown --id 1hJVjhXUNTs0GQP0MHH__sliYSkfEHgXE\n",
        "\n",
        "# download validation set\n",
        "!gdown --id 1tjXOTk_Ha1J7YqANFLYrw1ldb1IuOPr_"
      ],
      "metadata": {
        "id": "K_xXgPTzxgwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We unzip the .zip files into two directories - one for training, one for validation. These directories further contain a subdirectory each for horses and humans. These folders can be found in the tab to the left.\n",
        "\n",
        "We import the following libraries:\n",
        "\n",
        "\n",
        "1.   [os](https://docs.python.org/3/library/os.html) - perform operating system related tasks\n",
        "2.   [zipfile](https://docs.python.org/3/library/zipfile.html) - tools to manipulate zip files\n",
        "\n"
      ],
      "metadata": {
        "id": "4JEzrXDEohXQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLy3pthUS0D2"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "# training dataset\n",
        "local_zip = '/content/train-horse-or-human.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/content/train-horse-or-human')\n",
        "\n",
        "# validation dataset\n",
        "local_zip = '/content/validation-horse-or-human.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/content/validation-horse-or-human')\n",
        "\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory with our training horse pictures\n",
        "train_horse_dir = os.path.join('/content/train-horse-or-human/horses')\n",
        "# Directory with our training human pictures\n",
        "train_human_dir = os.path.join('/content/train-horse-or-human/humans')\n",
        "# Directory with our training horse pictures\n",
        "validation_horse_dir = os.path.join('/content/validation-horse-or-human/horses')\n",
        "# Directory with our training human pictures\n",
        "validation_human_dir = os.path.join('/content/validation-horse-or-human/humans')\n",
        "\n",
        "train_horse_names = os.listdir('/content/train-horse-or-human/horses')\n",
        "print(train_horse_names[:10])\n",
        "train_human_names = os.listdir('/content/train-horse-or-human/humans')\n",
        "print(train_human_names[:10])\n",
        "validation_horse_hames = os.listdir('/content/validation-horse-or-human/horses')\n",
        "print(validation_horse_hames[:10])\n",
        "validation_human_names = os.listdir('/content/validation-horse-or-human/humans')\n",
        "print(validation_human_names[:10])"
      ],
      "metadata": {
        "id": "ok7boXRAxCpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "**Question 1**: What's the best directory structure to store an image dataset for a classification task?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "cEy9asP6NyrI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display example images\n",
        "\n",
        "Let's visualize some training examples from each of the two classes.\n",
        "\n",
        "We import the following -\n",
        "1.   [matplotlib](https://matplotlib.org/) - visualization of data in Python"
      ],
      "metadata": {
        "id": "pjy36H2bK3Kz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "def show_img(path, file_name):\n",
        "    image = mpimg.imread(path + file_name)\n",
        "    plt.imshow(image)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "FB8PEO9dH41X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Displaying horse images\n",
        "\n"
      ],
      "metadata": {
        "id": "h0q2JbEfLU6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/train-horse-or-human/horses/'\n",
        "file_name = 'horse01-0.png'\n",
        "\n",
        "show_img(path, file_name)"
      ],
      "metadata": {
        "id": "Nb57PTmKLTMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Displaying human images"
      ],
      "metadata": {
        "id": "dN2EtbS-L2dQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# < YOUR CODE HERE >"
      ],
      "metadata": {
        "id": "6CbJBq99L6CV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cd7ECxpR3Vi"
      },
      "source": [
        "### Define model\n",
        "\n",
        "We define our model in the below cell which contains 4 convolution + maxpooling layers, followed by dense layers.\n",
        "\n",
        "We define our **input shape** to be 100 x 100 x 3 (3 channels representing RGB images). 100 x 100 is arbitrarily chosen (we have to ensure that the images fed to the neural network are of this shape - we might have to preprocess the original images to fit these dimensions).\n",
        "\n",
        "Our **output layer** contains only one neuron which outputs a value from 0 - 1 where '0' represents one class (say 'horses') and '1' represents the the other (say 'humans'). We use the **Sigmoid Activation Function** (A.F) to achieve this form of output. It is a non-linear activation function used for binary classification tasks. Read more about the Sigmoid A.F [here](https://machinelearningmastery.com/a-gentle-introduction-to-sigmoid-function/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PixZ2s5QbYQ3"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image with 3 bytes color\n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(100, 100, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fourth convolution\n",
        "    tf.keras.layers.Conv2D(256, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "**Question 2**: How does the output layer activation function vary for Binary and Multi-class Classification tasks?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "g2XS4UnEOkk7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the model's summary."
      ],
      "metadata": {
        "id": "J6nVZIhkmW5C"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMDg5mJxDrBq"
      },
      "source": [
        "# < YOUR CODE HERE >"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the below cell, we define the optimizer, loss function and metric!\n",
        "\n",
        "We import the following:\n",
        "1. RMSprop from [tensorflow.keras.optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers) - built-in optimizer classes\n",
        "\n",
        "Since there are only two classes, we use the ***binary cross entropy*** loss (as opposed to categorical cross entropy loss while classifying the MNIST dataset). Read more about binary cross entropy [here](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a).\n",
        "\n",
        "We use a different optimizer here - ***RMSprop*** - an adaptive learning based optimizer!"
      ],
      "metadata": {
        "id": "1ytzoappRrx0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DHWhFP_uhq3"
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "OPT = RMSprop(lr=0.0001)\n",
        "LOSS = 'binary_crossentropy'\n",
        "model.compile(loss=LOSS,\n",
        "              optimizer=OPT,\n",
        "              metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgON_hpSR3Vj"
      },
      "source": [
        "### Organize your data into Generators\n",
        "\n",
        "We import the following:\n",
        "1. ImageDataGenerator from [tensorflow.keras.preprocessing.image](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) - Utilies for image preprocessing and augmentation.\n",
        "\n",
        "We use the `ImageDataGenerator()` class function to augment images during training. The `flow_from_directory()` API takes a batch of training images from the specified path and applies random transformations to each image in the batch. Then, the Machine Learning model is trained on this transformed batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClebU9NJg99G"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# TRAINING\n",
        "# All images will be augmented according to whichever lines are uncommented below.\n",
        "# we can first try without any of the augmentation beyond the rescaling\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      #rotation_range=40,\n",
        "      #width_shift_range=0.2,\n",
        "      #height_shift_range=0.2,\n",
        "      #shear_range=0.2,\n",
        "      #zoom_range=0.2,\n",
        "      #horizontal_flip=True,\n",
        "      #fill_mode='nearest'\n",
        "      )\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/train-horse-or-human/',  # This is the source directory for training images\n",
        "        target_size=(100, 100),  # All images will be resized to 100x100\n",
        "        batch_size=128,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "\n",
        "# VALIDATION\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        '/content/validation-horse-or-human',\n",
        "        target_size=(100, 100),\n",
        "        class_mode='binary')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "**Question 3**: What is the purpose of Image Generators?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "fDPlE2gaVvRI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_doVZoHAR3Vk"
      },
      "source": [
        "### Train your model\n",
        "We now train our model. This may take a little while. Remember we are now building and training relatively complex computer vision models!\n",
        "\n",
        "The `model.fit()` API returns a history object whose history.history attribute is a record of training loss values and metrics values at successive epochs. You can read more about the API [here](https://www.tensorflow.org/api_docs/python/tf/keras/Model)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fb1_lgobv81m"
      },
      "source": [
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=8,\n",
        "      epochs=100,\n",
        "      verbose=1,\n",
        "      validation_data=validation_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6vSHzPR2ghH"
      },
      "source": [
        "### Run your Model\n",
        "\n",
        "We import the following:\n",
        "1. [numpy](https://numpy.org/) - scientific computing in Python\n",
        "2. files from [google.colab](https://colab.research.google.com/notebooks/io.ipynb) - loading and saving data from external sources.\n",
        "3. image from [keras.preprocessing](https://keras.io/api/preprocessing/) - Keras preprocessing utilities\n",
        "\n",
        "Let's now take a look at actually running a prediction using the model. This code will allow you to choose 1 or more files from your file system, it will then upload them, and run them through the model, giving an indication of whether the object is a horse or a human. **Was the model correct? Try a couple more images and see if you can confuse it!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoWp43WxJDNT"
      },
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "# from keras.preprocessing import image\n",
        "import keras.utils as image\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "\n",
        "  # predicting images\n",
        "  path = '/content/' + fn\n",
        "  img = image.load_img(path, target_size=(100, 100))\n",
        "  x = image.img_to_array(img)\n",
        "  x = x / 255.0\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  image_tensor = np.vstack([x])\n",
        "  classes = model.predict(image_tensor)\n",
        "  print(classes)\n",
        "  print(classes[0])\n",
        "  print()\n",
        "  if classes[0]>0.5:\n",
        "    print(fn + \" is a human!\")\n",
        "  else:\n",
        "    print(fn + \" is a horse!\")\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-9BosuqR3Vk"
      },
      "source": [
        "### Finally lets visualize all of the model layers!\n",
        "\n",
        "We import the following:\n",
        "1. [matplotlib](https://matplotlib.org/) - visualization of data in Python\n",
        "2. [random](https://docs.python.org/3/library/random.html) - pseudo-random number generation for various distributions\n",
        "\n",
        "Let's define a new Model that will take an image as input, and will output\n",
        "intermediate representations for all layers in the previous model after the first. We see the resolution of the images become lesser as we go deeper in the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-SUwB0bzVvc"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "\n",
        "successive_outputs = [layer.output for layer in model.layers[1:]]\n",
        "visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n",
        "# Let's prepare a random input image from the training set.\n",
        "horse_img_files = [os.path.join(train_horse_dir, f) for f in train_horse_names]\n",
        "human_img_files = [os.path.join(train_human_dir, f) for f in train_human_names]\n",
        "img_path = random.choice(horse_img_files + human_img_files)\n",
        "# uncomment the following line if you want to pick the Xth human file manually\n",
        "#img_path = human_img_files[0]\n",
        "\n",
        "img = load_img(img_path, target_size=(100, 100))  # this is a PIL image\n",
        "x = img_to_array(img)  # Numpy array with shape (100, 100, 3)\n",
        "x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 100, 100, 3)\n",
        "\n",
        "# Rescale by 1/255\n",
        "x /= 255.0\n",
        "\n",
        "# Let's run our image through our network, thus obtaining all\n",
        "# intermediate representations for this image.\n",
        "successive_feature_maps = visualization_model.predict(x)\n",
        "\n",
        "# These are the names of the layers, so can have them as part of our plot\n",
        "layer_names = [layer.name for layer in model.layers]\n",
        "\n",
        "# Now let's display our representations\n",
        "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
        "  if len(feature_map.shape) == 4:\n",
        "    # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
        "    n_features = feature_map.shape[-1]  # number of features in feature map\n",
        "    n_features = min(n_features,5) # limit to 5 features for easier viewing\n",
        "    # The feature map has shape (1, size, size, n_features)\n",
        "    size = feature_map.shape[1]\n",
        "    # We will tile our images in this matrix\n",
        "    display_grid = np.zeros((size, size * n_features))\n",
        "    for i in range(n_features):\n",
        "      # Postprocess the feature to make it visually palatable\n",
        "      x = feature_map[0, :, :, i]\n",
        "      x -= x.mean()\n",
        "      x /= x.std()\n",
        "      x *= 64\n",
        "      x += 128\n",
        "      x = np.clip(x, 0, 255).astype('uint8')\n",
        "      # We'll tile each filter into this big horizontal grid\n",
        "      display_grid[:, i * size : (i + 1) * size] = x\n",
        "    # Display the grid\n",
        "    scale = 20. / n_features\n",
        "    plt.figure(figsize=(scale * n_features, scale))\n",
        "    # plt.title(layer_name)\n",
        "    plt.grid(False)\n",
        "    plt.imshow(display_grid, aspect='auto', cmap='viridis')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "eF6IABEyTSh_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Binary Classification with Image Augmentation\n",
        "\n",
        "Now, we’ll again explore the horses v. humans dataset. This time, however, we will train with all forms of data augmentation turned on. You’ll see how this generates quite a bit more data (which takes longer to train) but produces a much more generalizable model since we are avoiding overfitting. You’ll get to test this by again having the chance to upload your own horse or human image and see if the model can detect it correctly. Finally you’ll again get to visualize the model layers.\n",
        "\n",
        "**Note:** Since we are training on a very limited dataset, with a very small model, the final model does make a decent amount of mistakes. You'll find that if you test with images that look closer to the training dataset it will work much better. This is because while we are avoiding some overfitting with data augmentation, all machine learning models will struggle to generalize far beyond the training dataset!"
      ],
      "metadata": {
        "id": "93MXWazQTLs5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "**Question 4**: How does Image Augmentation help w.r.t overfitting?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "rIYnwYf3UU1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now when we organize the data into Generators note how we use many more kinds of Data Augmentation!"
      ],
      "metadata": {
        "id": "eqEZb3VxTn80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be augmented with the full list of augmentation techniques below\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=20,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest'\n",
        "      )\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/content/train-horse-or-human/',  # This is the source directory for training images\n",
        "        target_size=(100, 100),  # All images will be resized to 300x300\n",
        "        batch_size=128,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        '/content/validation-horse-or-human',\n",
        "        target_size=(100, 100),\n",
        "        class_mode='binary')\n"
      ],
      "metadata": {
        "id": "a68m8ArbTpDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train your model with the new augmented data\n",
        "Since we now have more data due to the data augmentation this training process will take a bit longer than the last time. However, you'll find that the results are much better!"
      ],
      "metadata": {
        "id": "koxO9azdTrdk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=8,\n",
        "      epochs=100,\n",
        "      verbose=1,\n",
        "      validation_data=validation_generator)"
      ],
      "metadata": {
        "id": "GKocuSt5TwyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "**Question 5**: What do you observe about the training and validation accuracies when the model is trained with Image Augmentation?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ya-74RhRYBNQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's predict again\n",
        "\n",
        "Can you confuse it this time? Or did the extra data augmentation help the model generalize? What do you think it was about your confusing examples that are no longer confusing (or what is still confusing)?"
      ],
      "metadata": {
        "id": "bz5KHOi3T0QI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "# from keras.preprocessing import image\n",
        "import keras.utils as image\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "\n",
        "  # predicting images\n",
        "  path = '/content/' + fn\n",
        "  img = image.load_img(path, target_size=(100, 100))\n",
        "  x = image.img_to_array(img)\n",
        "  x = x / 255.0\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  image_tensor = np.vstack([x])\n",
        "  classes = model.predict(image_tensor)\n",
        "  print(classes)\n",
        "  print(classes[0])\n",
        "  if classes[0]>0.5:\n",
        "    print(fn + \" is a human\")\n",
        "  else:\n",
        "    print(fn + \" is a horse\")"
      ],
      "metadata": {
        "id": "qAgYt1ZtT1Yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finally again lets visualize some of the layers for intuition"
      ],
      "metadata": {
        "id": "ATt9U03nT53u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# Let's define a new Model that will take an image as input, and will output\n",
        "# intermediate representations for all layers in the previous model after the first.\n",
        "successive_outputs = [layer.output for layer in model.layers[1:]]\n",
        "visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n",
        "# Let's prepare a random input image from the training set.\n",
        "horse_img_files = [os.path.join(train_horse_dir, f) for f in train_horse_names]\n",
        "human_img_files = [os.path.join(train_human_dir, f) for f in train_human_names]\n",
        "img_path = random.choice(horse_img_files + human_img_files)\n",
        "# uncomment the following line if you want to pick the Xth human file manually\n",
        "# img_path = human_img_files[0]\n",
        "\n",
        "img = load_img(img_path, target_size=(100, 100))  # this is a PIL image\n",
        "x = img_to_array(img)  # Numpy array with shape (100, 100, 3)\n",
        "x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 100, 100, 3)\n",
        "\n",
        "# Rescale by 1/255\n",
        "x /= 255.0\n",
        "\n",
        "# Let's run our image through our network, thus obtaining all\n",
        "# intermediate representations for this image.\n",
        "successive_feature_maps = visualization_model.predict(x)\n",
        "\n",
        "# These are the names of the layers, so can have them as part of our plot\n",
        "layer_names = [layer.name for layer in model.layers[1:]]\n",
        "\n",
        "# Now let's display our representations\n",
        "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
        "  if len(feature_map.shape) == 4:\n",
        "    # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
        "    n_features = feature_map.shape[-1]  # number of features in feature map\n",
        "    n_features = min(n_features,5) # limit to 5 features for easier viewing\n",
        "    # The feature map has shape (1, size, size, n_features)\n",
        "    size = feature_map.shape[1]\n",
        "    # We will tile our images in this matrix\n",
        "    display_grid = np.zeros((size, size * n_features))\n",
        "    for i in range(n_features):\n",
        "      # Postprocess the feature to make it visually palatable\n",
        "      x = feature_map[0, :, :, i]\n",
        "      x -= x.mean()\n",
        "      x /= x.std()\n",
        "      x *= 64\n",
        "      x += 128\n",
        "      x = np.clip(x, 0, 255).astype('uint8')\n",
        "      # We'll tile each filter into this big horizontal grid\n",
        "      display_grid[:, i * size : (i + 1) * size] = x\n",
        "    # Display the grid\n",
        "    scale = 20. / n_features\n",
        "    plt.figure(figsize=(scale * n_features, scale))\n",
        "    #plt.title(layer_name)\n",
        "    plt.grid(False)\n",
        "    plt.imshow(display_grid, aspect='auto', cmap='viridis')"
      ],
      "metadata": {
        "id": "ler2bzKDT628"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean Up\n",
        "\n",
        "Before running the next exercise, run the following cell to terminate the kernel and free memory resources:"
      ],
      "metadata": {
        "id": "Q2ie0seOUA1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, signal\n",
        "os.kill(os.getpid(), signal.SIGKILL)"
      ],
      "metadata": {
        "id": "V1nLN7YUUDyg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}