{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "Before starting, you must click on the \"Copy To Drive\" option in the top bar. Go to File --> Save a Copy to Drive. Name it *'LastName_FirstName_T2.1.ipynb'*. <ins>This is the master notebook so you will not be able to save your changes without copying it !</ins> Once you click on that, make sure you are working on that version of the notebook so that your work is saved.\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "3tDIhF6tplj5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In this Colab, we will explore the *TensorFlow Lite Converter* to create a TFLite model from the original TensorFlow model."
      ],
      "metadata": {
        "id": "rIDgevmQ1ONM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1J15Vh_1Jih",
        "cellView": "both",
        "outputId": "820ef0a6-eee4-4af7-a3b9-2a6a58b68e83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import the following libraries -\n",
        "\n",
        "1.   [tensorflow](https://www.tensorflow.org/) - open source library to develop and train ML models\n",
        "2.   [numpy](https://numpy.org/) - scientific computing in Python\n",
        "3.   [keras](https://keras.io/) - deep learning framework built on top of TensorFlow"
      ],
      "metadata": {
        "id": "KRoL4QUT-7Ye"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6k2Pg0gTYB8"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Define TensorFlow Model\n",
        "\n",
        "We consider a single-layer neural network for a Linear Regression Task - modeling Y = 2X + 3. We define the optimizer to be 'Stochastic Gradient Descent' and the loss function to be 'Mean Squared Error'."
      ],
      "metadata": {
        "id": "Zh7QdIbg_Gxe"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAwdHf6ySQGt"
      },
      "source": [
        "my_layer = Dense(units=1, input_shape=[1])\n",
        "model = Sequential([my_layer])\n",
        "\n",
        "OPT = # < YOUR CODE HERE >\n",
        "LOSS = # < YOUR LOSS HERE >\n",
        "model.compile(optimizer=OPT, loss=LOSS)\n",
        "\n",
        "xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
        "ys = np.array([1.0, 3.0, 5.0, 7.0, 9.0, 11.0], dtype=float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Hint:*** Use optimizer='sgd', loss='mean_squared_error'"
      ],
      "metadata": {
        "id": "ejK-B-OrcNJf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Model\n",
        "\n",
        "We fit our neural network on the defined data (Xs and Ys)."
      ],
      "metadata": {
        "id": "S6rTaMfAAh8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(xs, ys, epochs=500)"
      ],
      "metadata": {
        "id": "e_0VRfW-Al2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict X = 10.0 and inspect learned weights\n",
        "\n",
        "We evaluate our trained model by tasking it to predict the output (Y) for input (X) = 10.0! We further observe the model parameters (weight and bias)"
      ],
      "metadata": {
        "id": "zRRmNnEEAmtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.predict([10.0]))\n",
        "\n",
        "print(\"Here are the parameters that the model learned: {}\".format(my_layer.get_weights()))"
      ],
      "metadata": {
        "id": "av_0eeHxAt2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save Model\n",
        "\n",
        "We save our trained model to a specified path. This is done using the `saved_model.save()` API that exports a TensorFlow model object to SavedModel format. You can read more about the API [here](https://www.tensorflow.org/api_docs/python/tf/saved_model).\n",
        "\n",
        "The saved model (.pb file) can be found on the left tab under Files/.  "
      ],
      "metadata": {
        "id": "q9QbMXDrDk5G"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOs_IDM6ToaM"
      },
      "source": [
        "export_dir = 'saved_TF_model/model1'\n",
        "tf.saved_model.save(model, export_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Convert TF Model to TFLite Model\n",
        "\n",
        "We use the `tf.lite.TFLiteConverter.from_saved_model()` API to load the saved TF model and then use the `convert()` method to transform into the TFLite model. You can read more about the API [here](https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter)."
      ],
      "metadata": {
        "id": "CxJri4pAIDtg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWSlkprhTsWE"
      },
      "source": [
        "# Convert the model.\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)     # instantiating the converter object\n",
        "tflite_model = converter.convert()                                   # converting TF model to TF Lite model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Storing TFLite model onto disk\n",
        "\n",
        "We import the following:\n",
        "\n",
        "1. [pathlib](https://docs.python.org/3/library/pathlib.html) - Object-oriented filesystem paths\n",
        "\n",
        "We write the TFLite model (model.tflite) to the disk - this file encapsulates the model and its saved weights. This is the compressed model that we deploy on edge devices!\n",
        "\n",
        "\\\n",
        "\n",
        "We first create the desired path using the `pathlib.Path()` API. We then write the contents from our converted tflite model to this path using the `tflite_model_file.write_bytes()` API. You can read more about this API [here.](https://www.tensorflow.org/lite/performance/post_training_quant)\n",
        "\n"
      ],
      "metadata": {
        "id": "6IGiHv_1K5H_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsaEjJfrTujk"
      },
      "source": [
        "import pathlib\n",
        "tflite_model_file = pathlib.Path('model.tflite')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Size of TensorFlow Lite Model in bytes:\", tflite_model_file.write_bytes(tflite_model)  )   #output = no. of bytes of model"
      ],
      "metadata": {
        "id": "cuO6xgORMK_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `tflite_model_file.write_bytes()` API also returns the size of the model in bytes. The *model.tflite* file can be found on the left tab under File/"
      ],
      "metadata": {
        "id": "yPB7H8qlL_pk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Question 1**: Why do we convert our TensorFlow model to a TensorFlow Lite model for edge devices?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "JRFYQef7imaZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Inference using TensorFlow Interpreter\n",
        "\n",
        "Now that we have a TFLite model, how do we test it out without actually needing an edge device?\n",
        "\n",
        "We don't need a RPi, smartphone, or an embedded system! TensorFlow Lite provides an interface called **Interpreter** that allows us to load the TF Lite model and run inference in Python. You can read more about the TFLite Interpreter [here](https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter).\n",
        "\n",
        "We use the `tf.lite.Interpreter()` API to load the tflite model to the Interpreter and we extract the input and output tensor details to run inference. The input and output details help us define important details such as the shape of our input for inference and expected output after inference. It also defines the index of the interpreter assigned for the input and output tensors.  "
      ],
      "metadata": {
        "id": "6JxcMWxvMQ0Z"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fseX4pkhTzS0"
      },
      "source": [
        "# Load TFLite model and allocate tensors.\n",
        "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensors - to run inference\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "print(input_details)\n",
        "print(output_details)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then format our value to be predicted (10.0) as a tensor according to how the model accepts input, run inference, and read the result on the output tensor and store it in a variable (here, tflite_results)."
      ],
      "metadata": {
        "id": "a5cq5oRA0kcH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_Ij8_BvU0KV"
      },
      "source": [
        "to_predict = np.array([[10.0]], dtype=np.float32)   # based on 'shape' value in input details\n",
        "print(\"value to predict:\", to_predict)\n",
        "\n",
        "# setting the 'to_predict' value at the appropriate index location of input tensor\n",
        "interpreter.set_tensor(input_details[0]['index'], to_predict)\n",
        "\n",
        "# running inference on 'to_predict' value\n",
        "interpreter.invoke()\n",
        "\n",
        "#read the result that is stored at the appropriate index location of output tensor\n",
        "tflite_results = interpreter.get_tensor(output_details[0]['index'])\n",
        "print(\"predicted value:\", tflite_results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Question 2**: List any two reasons why we require the model input and output tensor details.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "KOHAR9qJnZik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use the interepreter to predict X = 5.5\n",
        "\n",
        "In the code cell below, use the already instantiated 'Interpreter' to predict X = 5.5!"
      ],
      "metadata": {
        "id": "Mic1Py8wpH9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "\n"
      ],
      "metadata": {
        "id": "WLKbX7ECp8Ne"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}