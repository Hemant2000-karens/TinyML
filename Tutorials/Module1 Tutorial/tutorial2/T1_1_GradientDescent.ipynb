{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "Before starting, you must click on the \"Copy To Drive\" option in the top bar. Go to File --> Save a Copy to Drive. Name it *'LastName_FirstName_T1.1.ipynb'*. <ins>This is the master notebook so you will not be able to save your changes without copying it !</ins> Once you click on that, make sure you are working on that version of the notebook so that your work is saved.\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "SfhF5BUcomZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In this Colab notebook we explore loss - in particular, Root Mean Squared Error (RMSE) Loss for a Regression problem. We also train a basic Machine Learning model using Gradient Descent to fit our data better and consequently minimize the loss.\n"
      ],
      "metadata": {
        "id": "DUtcejBhPRi3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.Exploring Loss"
      ],
      "metadata": {
        "id": "-TiTRaOfOlIq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we define a Linear Regression problem of the form Y = wX + b. We are provided with data (xs and ys) using which we need to infer the relationship between x and y i.e., arrive at the accurate values of weight (w) and bias (b)."
      ],
      "metadata": {
        "id": "SQxFbTjb1RRf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Question 1**: It is said that in linear regression, we try to find the best fit line. Can you think of why?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Oo4qroNybu_H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import two libraries -\n",
        "\n",
        "\n",
        "1.   [math](https://docs.python.org/3/library/math.html) - basic mathematical operations. eg: square root, power etc.\n",
        "2.   [matplotlib](https://matplotlib.org/) - visualization of data in Python\n",
        "\n"
      ],
      "metadata": {
        "id": "Rr3SfU1pOHji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import math\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ULRbhByH_g6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below cell computes the predicted Y values for your guess of w and b. Simply change their values and explore how the output and subsequently the loss changes."
      ],
      "metadata": {
        "id": "mzT77SGLNmzq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "**Question 2**: Report the loss for three different values of 'w' and 'b'. (Run the below three cells for each example)\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "\n",
        "1.   w = , b = , loss =\n",
        "2.   w = , b = , loss =\n",
        "3.   w = , b = , loss =\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "3vI1oUZPL1QL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwrR2q7tZKPi",
        "outputId": "5abb5ef8-c87b-44ba-ce2f-0a5df818635a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Real Y is [-3, -1, 1, 3, 5, 7]\n",
            "My Y is   [-3, -1, 1, 3, 5, 7]\n"
          ]
        }
      ],
      "source": [
        "# Edit these parameters w and b to try different loss\n",
        "# measurements. Rerun this cell when done\n",
        "# Your Y will be calculated as Y = wX + b, so\n",
        "# if w = 3, and b = -1, then Y = 3x - 1\n",
        "\n",
        "w = 2# < YOUR CODE HERE >\n",
        "b = -1# < YOUR CODE HERE >\n",
        "\n",
        "x = [-1, 0, 1, 2, 3, 4]\n",
        "y = [-3, -1, 1, 3, 5, 7]     # ground truth for w = 2, b = -1, i.e, Y = 2x - 1\n",
        "myY = []\n",
        "\n",
        "\n",
        "for thisX in x:              # predicted Ys based on your parameters\n",
        "  thisY = (w*thisX)+b\n",
        "  myY.append(thisY)\n",
        "\n",
        "print(\"Real Y is \" + str(y))\n",
        "print(\"My Y is   \" + str(myY))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we move on to calculating the loss, lets visualize the difference between the actual Y and predicted Y values. In the below scatter plot, the *blue dots* represent the actual Y values, the *green dots* represent the predicted Y values and the *red dashed lines* represent the difference between the two for each individual data sample.\n",
        "\n",
        "You can read more about matplotlib scatter plots [here](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html)."
      ],
      "metadata": {
        "id": "2759rVV6CjFY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJxSeN1B-fL8",
        "outputId": "cda8dbff-21c5-46b8-cf9d-702a0f79d64f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# visualize loss/errors\n",
        "plt.scatter(x, y, color='blue')                  # ground truth\n",
        "plt.scatter(x, myY, color='green')               # predicted\n",
        "plt.vlines(x, ymin=min(y,myY), ymax=max(y,myY), color=\"red\", linestyle='--', alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "**Question 3**: What do the 2nd and 3rd parameters of the `plt.vlines()` API signify?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "9Py5d35ZM_KY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We could just take the average of the differences between the actual and predicted Ys (errors) to calculate the loss of the system. However, the presence of negative errors would cause the errors to cancel out, not giving an accurate representation of the loss. Therefore, the smart thing to do here is to square each error amount, average out all the individual squared errors, and then take the square root of that - **RMSE Loss**\n",
        "\n",
        "**Note:** Some other measures are Mean Squared Error (MSE) Loss and Mean Absolute Error (MAE) Loss."
      ],
      "metadata": {
        "id": "2oUxMYFcFe6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's calculate the loss\n",
        "total_square_error = 0\n",
        "size = len(y)\n",
        "for i in range(0, size):\n",
        "  square_error = (y[i] - myY[i]) ** 2\n",
        "  total_square_error += square_error\n",
        "\n",
        "total_square_error /= size\n",
        "\n",
        "print(\"My loss is: \" + str(math.sqrt(total_square_error)))"
      ],
      "metadata": {
        "id": "MUOgz0NoChZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Minimizing Loss"
      ],
      "metadata": {
        "id": "YZqMW2IHvqXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we attempt to iteratively minimize the loss function of our regression model by training the model on given data over multiple epochs."
      ],
      "metadata": {
        "id": "LOr5Jm5TwZKr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import the following libraries -\n",
        "\n",
        "\n",
        "1.   [tensorflow](https://www.tensorflow.org/) - open source library to develop and train ML models\n",
        "2.   [numpy](https://numpy.org/) - scientific computing in Python\n",
        "3.   [matplotlib](https://matplotlib.org/) - visualization of data in Python\n",
        "\n"
      ],
      "metadata": {
        "id": "sGS5IutpPhGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "l8uQAtpf_smh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47ef6b9a-0827-4b34-a78e-991d1d4485ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train our Linear Regression Model, we need to first define our (i) ML Model, (ii) Loss Function, and the (iii) Training Procedure."
      ],
      "metadata": {
        "id": "FDgvPAZZQt2e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression Model\n",
        "\n",
        "The model represents the function Y = wX + b. Therefore, the model outputs a value of Y for a given w, b, and X when it is called."
      ],
      "metadata": {
        "id": "SLCbfuwWSTSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define our initial guess\n",
        "INITIAL_W = 10.0\n",
        "INITIAL_B = 10.0\n",
        "\n",
        "# Define our simple regression model\n",
        "class Model(object):\n",
        "  def __init__(self):\n",
        "    # Initialize the weights\n",
        "    self.w = tf.Variable(INITIAL_W)\n",
        "    self.b = tf.Variable(INITIAL_B)t\n",
        "\n",
        "  def __call__(self, x):\n",
        "    return self.w*x +self.b# <YOUR CODE HERE> #"
      ],
      "metadata": {
        "id": "JOKXLdtQYAn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Hint:*** Given X, the `__call__()` class method should return Y. We know Y = wX + b. Therefore, return `self.w * x + self.b`"
      ],
      "metadata": {
        "id": "yaSBNf3JN9PM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss Function\n",
        "\n",
        "We then define our loss function which returns the Mean Squared Error (MSE) Loss.\n",
        "\n",
        "The tensorflow API `tf.reduce_mean()` computes the mean of the input tensor. It reduces the input tensor along the specified axis. Read more [here](https://www.tensorflow.org/api_docs/python/tf/math/reduce_mean)."
      ],
      "metadata": {
        "id": "qZyKjNZkbMXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define our loss function\n",
        "def loss(predicted_y, target_y):\n",
        "  return tf.reduce_mean(tf.square(predicted_y - target_y))"
      ],
      "metadata": {
        "id": "jY0OplVNbWHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Procedure\n",
        "\n",
        "Here, Gradient Descent optimization is used for the training procedure. We define a train function which performs one step of **Gradient Descent** on all data samples in our dataset (i.e., all Xs and Ys). The Gradient Descent Optimization Algorithm involves:\n",
        "\n",
        "1.   the calculation of the loss function\n",
        "2.   the derivatves of the loss function w.r.t the individual parameters (w and b here)\n",
        "3.   updation of model parameters with the calculated derivatives\n",
        "\n",
        "You can read more about the math behind Gradient Descent [here](https://towardsdatascience.com/understanding-the-mathematics-behind-gradient-descent-dde5dc9be06es://). It is however beyond the scope of the course."
      ],
      "metadata": {
        "id": "cJj1cxFadTDy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TensorFlow API Gradient Tape** -\n",
        "\n",
        "The Calculus is managed by a TensorFlow Gradient Tape. You can learn more about the gradient tape at https://www.tensorflow.org/api_docs/python/tf/GradientTape"
      ],
      "metadata": {
        "id": "aDxqkMRskxx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define our training procedure\n",
        "def train(model, inputs, outputs, learning_rate):\n",
        "  with tf.GradientTape() as t:\n",
        "    # loss function\n",
        "    current_loss = loss(model(inputs), outputs)\n",
        "\n",
        "    # Here is where you differentiate loss function w.r.t model parameters\n",
        "    dw, db = t.gradient(current_loss, [model.w, model.b])      # dloss/dw, dloss/db\n",
        "\n",
        "    # And here is where you update the model parameters based on the learning rate chosen\n",
        "    model.w.assign_sub(learning_rate * dw)   # model.w = model.w - learning_rate*dw\n",
        "    # <YOUR CODE HERE>   # model.b = model.b - learning_rate*db\n",
        "    return current_loss"
      ],
      "metadata": {
        "id": "9fLQyjzVv4u3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "**Question 4**: In the above example, before we train our model, what are the three things that we need to define?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "-1qR8Ly_SA7I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Model\n",
        "\n",
        "Now that we have defined the above individual functions, its time to put them together.\n",
        "\n",
        "We first define our dataset, then instantiate our model. We then train our model over multiple epochs. For each epoch, we calculate and store the parameter values (w and b) and the loss."
      ],
      "metadata": {
        "id": "t9OVMTeQwIYy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice the decrease in loss as we go through more epochs."
      ],
      "metadata": {
        "id": "PktVRvVFTWQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define our input data and learning rate\n",
        "xs = [-1.0, 0.0, 1.0, 2.0, 3.0, 4.0]\n",
        "ys = [-3.0, -1.0, 1.0, 3.0, 5.0, 7.0]\n",
        "LEARNING_RATE = 0.14   #0.09\n",
        "\n",
        "# Instantiate our model\n",
        "model = Model()\n",
        "\n",
        "# Collect the history of w-values and b-values to plot later\n",
        "list_w, list_b = [], []\n",
        "epochs = 50\n",
        "losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  list_w.append(model.w.numpy())\n",
        "  list_b.append(model.b.numpy())\n",
        "  current_loss = train(model, xs, ys, learning_rate=LEARNING_RATE)\n",
        "  losses.append(current_loss)\n",
        "  print('Epoch %2d: w=%1.2f b=%1.2f, loss=%2.5f' %\n",
        "        (epoch, list_w[-1], list_b[-1], current_loss))"
      ],
      "metadata": {
        "id": "NCPlW9FpwFgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot our trained values over time\n",
        "\n",
        "Below is a plot of the model parameter values w and b (y-axis) across all the epochs (x-axis). As the model is trained, notice how the values of w and b eventually converge to the true values 2.0 and -1.0 respectively."
      ],
      "metadata": {
        "id": "6afLtlyRwR-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the w-values and b-values for each training Epoch against the true values\n",
        "TRUE_w = 2.0\n",
        "TRUE_b = -1.0\n",
        "xaxis = range(epochs)\n",
        "plt.plot(xaxis, list_w, 'r', xaxis, list_b, 'b')\n",
        "plt.plot([TRUE_w] * epochs, 'r--', [TRUE_b] * epochs, 'b--')\n",
        "plt.legend(['w', 'b', 'True w', 'True b'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "50uecDpHwPGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "**Question 5**: In the above plot, what do the solid lines and dotted lines signify?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "XcD48DftWR1S"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}