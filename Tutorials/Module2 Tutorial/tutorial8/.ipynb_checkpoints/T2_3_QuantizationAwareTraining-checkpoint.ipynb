{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "Before starting, you must click on the \"Copy To Drive\" option in the top bar. Go to File --> Save a Copy to Drive. Name it *'LastName_FirstName_T2.3.ipynb'*. <ins>This is the master notebook so you will not be able to save your changes without copying it !</ins> Once you click on that, make sure you are working on that version of the notebook so that your work is saved.\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "e9e2QA9vp1EX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuOe1ymfHZPu"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In this tutorial, we will explore Quantization Aware Training! In the first section, we'll create a model for the MNIST Dataset and fine tune it to be Quantization Aware. In the second section, we'll convert this fine-tuned model to a Quantized TFLite model and observe the persistence of accuracy. Finally, in sections 3 and 4 we will compare model performance and model size of the following TFLite models - (i) Baseline TFLite model, (ii) Quantized TFLite model, and (iii) Quantized TFLite model with Quantization Awareness"
      ],
      "metadata": {
        "id": "82Cp6XuXQqeA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN4yVFK5-0Bf"
      },
      "source": [
        "! pip install -q tensorflow-model-optimization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import the following -\n",
        "\n",
        "\n",
        "1.    [tensorflow](https://www.tensorflow.org/) - open source library to develop and train ML models\n",
        "2.   [tempfile](https://docs.python.org/3/library/tempfile.html) - to create temporary files and directories\n",
        "3. [os](https://docs.python.org/3/library/os.html) - perform operating system related tasks\n",
        "\n"
      ],
      "metadata": {
        "id": "wwqd4IXnsu6i"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJwIonXEVJo6"
      },
      "source": [
        "import tempfile\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Fine-tune Baseline Model to become Quantization Aware"
      ],
      "metadata": {
        "id": "fthXWB-qWsBI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psViY5PRDurp"
      },
      "source": [
        "### Load Dataset\n",
        "\n",
        "Here, we will define our baseline model to classify the 10 different digits from the MNIST Dataset. We will load the dataset, split it into train and test datasets, and normalize our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbY-KGMPvbW9"
      },
      "source": [
        "# Load MNIST dataset\n",
        "mnist = keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Normalize the input image so that each pixel value is between 0 to 1.\n",
        "train_images = # < YOUR CODE HERE >\n",
        "test_images =  # < YOUR CODE HERE >\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Baseline Model\n",
        "The model architecture is defined below. The model takes in (28, 28) greyscale images and applies a convolution and maxpool layer. The final layer has 10 neurons, reflecting the number of classes in our dataset.\n",
        "\n",
        "We then compile our model."
      ],
      "metadata": {
        "id": "eVZqyRBOVZYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model architecture.\n",
        "model = keras.Sequential([\n",
        "  keras.layers.InputLayer(input_shape=(28, 28)),                                    # Input layer accepts (28, 28) and converts to (28, 28, 1)\n",
        "  keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
        "  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
        "  keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "  keras.layers.Flatten(),\n",
        "  keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# baseline model - 'model' (model 1)"
      ],
      "metadata": {
        "id": "DWgyToBhVPJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "**Question 1**: Why do we not define a 'softmax' activation function in our final layer of our model?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "rdT57rscXPIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Baseline Model\n",
        "\n",
        "We train our model for 1 epoch."
      ],
      "metadata": {
        "id": "cAQz06BCWeFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the digit classification model\n",
        "\n",
        "model.fit(\n",
        "  train_images,\n",
        "  train_labels,\n",
        "  epochs=1,\n",
        "  validation_split=0.1,\n",
        ")"
      ],
      "metadata": {
        "id": "zlCoMhLsUiM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "rKA8Wf3rUvPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8747K9OE72P"
      },
      "source": [
        "### Fine-Tune Baseline Model with Quantization Aware Training\n",
        "\n",
        "You will apply quantization aware training to the baseline model we created above. This allows the model to learn parameters robust to quantization loss, and also model the accuracy of a quantized model. We will observe this change in the model summary - the layers are now prefixed by \"quant\". This is done using the `tfmot.quantization.keras.quantize_model()` API. You can read more about this API [here](https://www.tensorflow.org/model_optimization/api_docs/python/tfmot/quantization/keras/quantize_model).\n",
        "\n",
        "***Note1:***  The resulting model is quantization aware but not quantized (e.g. the weights are float32). The sections that follow show how to create a quantized model from the quantization aware one.\n",
        "\n",
        "***Note2:*** In the [comprehensive guide](https://www.tensorflow.org/model_optimization/guide/quantization/training_comprehensive_guide.md), you can see how to quantize some layers for model accuracy improvements.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries\n",
        "\n",
        "We import the following library -\n",
        "\n",
        "\n",
        "1.   [tensorflow_model_optimization](https://www.tensorflow.org/model_optimization) - suite of tools for optimizing ML models for deployment and execution\n",
        "\n"
      ],
      "metadata": {
        "id": "QvUwJeZdaNrG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oq6blGjgFDCW"
      },
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "# instantiate API\n",
        "quantize_model = tfmot.quantization.keras.quantize_model\n",
        "\n",
        "# create quantization aware model\n",
        "q_aware_model = # <ENTER YOUR CODE HERE>\n",
        "\n",
        "\n",
        "\n",
        "# quantization aware model - q_aware_model (model 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Hint:*** Use `quantize_model(model)`"
      ],
      "metadata": {
        "id": "lQ5ZdVX5uiQh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We recompile our fine-tuned model!"
      ],
      "metadata": {
        "id": "xQFP5JTfZy6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# `quantize_model` requires a recompile.\n",
        "q_aware_model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "21tb03NbZyFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary of Quantization Aware Model!"
      ],
      "metadata": {
        "id": "G4Y9hEfbZ6pQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# < YOUR CODE HERE >"
      ],
      "metadata": {
        "id": "ncEW9mcoZ50E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDr2ijwpGCI-"
      },
      "source": [
        "### Train Fine-Tuned Model\n",
        "To demonstrate the power of quantization-aware training, we train our fine-tuned model only on a subset of the training data. We see that the fine-tuned model does very well on the test dataset even though it is trained only on a subset of the dataset. We train for 1 epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PHDGJryE31X"
      },
      "source": [
        "train_images_subset = train_images[0:1000] # out of 60000\n",
        "train_labels_subset = train_labels[0:1000]\n",
        "\n",
        "# train our quantization-aware model\n",
        "q_aware_model.fit(train_images_subset, train_labels_subset,\n",
        "                  batch_size=500, epochs=1, validation_split=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-byC2lYlMkfN"
      },
      "source": [
        "### Evaluate both models on Test Dataset\n",
        "\n",
        "We see that for this example, there is minimal to no loss in test accuracy after quantization aware training, compared to the baseline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bMFTKSSHyyZ"
      },
      "source": [
        "_, baseline_model_accuracy = model.evaluate(test_images, test_labels, verbose=0)\n",
        "\n",
        "_, q_aware_model_accuracy = q_aware_model.evaluate(test_images, test_labels, verbose=0)\n",
        "\n",
        "print('Baseline test accuracy:', baseline_model_accuracy)\n",
        "print('Quant-Aware test accuracy:', q_aware_model_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "**Question 2**: What is the advantage of Quantization Aware Training?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "7-neXhEvcTJl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IepmUPSITn6"
      },
      "source": [
        "# 2. Create Quantized Model for TFLite backend\n",
        "\n",
        "In this section, we will convert our fine-tuned (quantization aware) model to a TFLite model with post-training quantization (Dynamic Range Quantization). After this, we have an actually quantized model with **8-bit precision** values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7fztWsAOHTz"
      },
      "source": [
        "# instantiate converter\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# Convert the model\n",
        "quantized_QA_tflite_model = # < YOUR CODE HERE >\n",
        "\n",
        "\n",
        "\n",
        "# quantized model with quantization awareness - quantized_QA_tflite_model (model 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Hint:*** Use the `converter.convert()` method."
      ],
      "metadata": {
        "id": "AerBUhiafXTF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "**Question 3**: How is `tf.lite.TFLiteConverter.from_keras_model()` API different from the `tf.lite.TFLiteConverter.from_saved_model()` API we used in previous tutorials? Can you think of a disadvantage of the former method?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "zuMZe7iSfh4w"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEYsyYVqNgeY"
      },
      "source": [
        "### Observe Persistence of Accuracy from TF to TFLite\n",
        "We use this Quantized TFLite model to run inference on our test dataset and compare the test accuracy to that of the Quantization Aware TensorFlow model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first define a helper function to evaluate our TFLite model's performance on the test dataset. Remember, we have to use an **Interpreter** to run inference of our TFlite model on Colab. The `evaluate_model()` function defined below takes in an instance of the interpreter."
      ],
      "metadata": {
        "id": "EgHP4i_iiwLT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8yBouuGNqls"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def evaluate_model(interpreter):\n",
        "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "  # Run predictions on every image in the \"test\" dataset.\n",
        "  prediction_digits = []\n",
        "  for i, test_image in enumerate(test_images):\n",
        "    if i % 1000 == 0:\n",
        "      print('Evaluated on {n} results so far.'.format(n=i))\n",
        "\n",
        "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "    # the model's input data format.\n",
        "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "    interpreter.set_tensor(input_index, test_image)\n",
        "\n",
        "    # Run inference.\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Post-processing: remove batch dimension and find the digit with highest\n",
        "    # probability.\n",
        "    output = interpreter.tensor(output_index)\n",
        "    digit = np.argmax(output()[0])\n",
        "    prediction_digits.append(digit)\n",
        "\n",
        "  print('\\n')\n",
        "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
        "  prediction_digits = np.array(prediction_digits)\n",
        "  accuracy = (prediction_digits == test_labels).mean()\n",
        "  return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuEFS4CIQvUw"
      },
      "source": [
        "In the below cell, we instantiate our Interpreter. We evaluate the quantized TFLite model and see that the accuracy from TensorFlow persists to the TFLite backend."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqQTyqz4NsWd"
      },
      "source": [
        "# instantiate interpreter\n",
        "interpreter = tf.lite.Interpreter(model_content=quantized_QA_tflite_model)\n",
        "\n",
        "\n",
        "TFLite_quant_QA_accuracy = evaluate_model(interpreter)\n",
        "\n",
        "\n",
        "print('Quant TensorFlow test accuracy:', q_aware_model_accuracy)\n",
        "print('Quant TFLite with QA test accuracy:', TFLite_quant_QA_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "**Question 4**: Why do we allocate tensors to our Interpreter?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "65L_o0rJm1-V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Compare TFLite Model Performances\n",
        "\n",
        "In this section, we will compare the performance of (i) Baseline TFLite model, (ii) Quantized TFLite model, and (iii) Quantized TFLite model with Quantization Awareness on the MNIST Test Dataset.\n",
        "\n",
        "We already have the test accuracy of the Quantized TFLite model with Quantization Awareness in 'TFLite_QA_test_accuracy' (from Section 2). For the Baseline TFLite model and Quantized TFLite model we convert the baseline TensorFlow model using TFLiteConverter without and with optimization respecively.  "
      ],
      "metadata": {
        "id": "6pyErmCS2hdp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(i) Let's create our Baseline TFLite model! This TFLite model has no Quantization Awareness and no Post-Training Quantization."
      ],
      "metadata": {
        "id": "Gzc5s_feGPVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create float TFLite model.\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)      # 'model' is our baseline model created in section 1\n",
        "baseline_tflite_model = converter.convert()\n",
        "\n",
        "\n",
        "\n",
        "# baseline TFLite model with no quantization awareness and no post-training quantization - baseline_tflite_model (model 4)"
      ],
      "metadata": {
        "id": "AxjFasTYp99j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's evaluate this model on the test dataset and calculate the test accuracy. We instantiate our Interpreter and run inference using our helper function - `evaluate_model()`"
      ],
      "metadata": {
        "id": "8t5cuXz65qjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate interpreter\n",
        "interpreter = tf.lite.Interpreter(model_content=baseline_tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "TFLite_baseline_accuracy = evaluate_model(interpreter)\n"
      ],
      "metadata": {
        "id": "0FsmuRtV6AR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(ii) Let's create our Quantized TFLite model! This TFLite model has Post-Training Quantization but no Quantization Awareness."
      ],
      "metadata": {
        "id": "iHciffssHPfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate converter\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = # < YOUR CODE HERE >\n",
        "\n",
        "# Convert the model\n",
        "quantized_tflite_model = converter.convert()\n",
        "\n",
        "\n",
        "\n",
        "# quantized TFLite model without quantization awareness - quantized_tflite_model (model 5)"
      ],
      "metadata": {
        "id": "y737Bl755G0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Hint:*** Use `[tf.lite.Optimize.DEFAULT]`"
      ],
      "metadata": {
        "id": "8bCnS4x2I0FN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's evaluate this model on the test dataset and calculate the test accuracy. We instantiate our Interpreter and run inference using our helper function - `evaluate_model()`"
      ],
      "metadata": {
        "id": "LUB1DGWEIIih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO : instantiate interpreter and save accuracy to 'TFLite_quant_accuracy'\n",
        "\n"
      ],
      "metadata": {
        "id": "sMGODbbQG5_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now compare the test accuracies of the TFLite models!"
      ],
      "metadata": {
        "id": "FFR2Kqm_IJqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Baseline TFLite model test accuracy:', TFLite_baseline_accuracy)\n",
        "print('Quant TFLite model test accuracy:', TFLite_quant_accuracy)\n",
        "print('Quant TFLite model with QA test accuracy:', TFLite_quant_QA_accuracy)\n"
      ],
      "metadata": {
        "id": "gDLvyyeuIO6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8D7WnFF5DZR"
      },
      "source": [
        "# 4. Compare TFLite Model Sizes\n",
        "\n",
        "In this section we compare the model sizes of (i) Baseline TFLite model, (ii) Quantized TFLite model, and (iii) Quantized TFLite model with Quantization Awareness."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jy_Lgfh8VkyX"
      },
      "source": [
        "# Measure sizes of models.\n",
        "_, baseline_file = tempfile.mkstemp('.tflite')\n",
        "_, quant_file = tempfile.mkstemp('.tflite')\n",
        "_, quant_QA_file = tempfile.mkstemp('.tflite')\n",
        "\n",
        "\n",
        "with open(baseline_file, 'wb') as f:\n",
        "  f.write(baseline_tflite_model)\n",
        "\n",
        "with open(quant_file, 'wb') as f:\n",
        "  f.write(quantized_tflite_model)\n",
        "\n",
        "with open(quant_QA_file, 'wb') as f:\n",
        "  f.write(quantized_QA_tflite_model)\n",
        "\n",
        "\n",
        "TFLite_baseline_size = os.path.getsize(baseline_file) / float(2**20)\n",
        "TFLite_quant_size = os.path.getsize(quant_file) / float(2**20)\n",
        "TFLite_quant_QA_size = os.path.getsize(quant_QA_file) / float(2**20)\n",
        "\n",
        "print(\"Baseline model in Mb:\", TFLite_baseline_size)             # Baseline TFLite model\n",
        "print(\"Quantized model in Mb:\", TFLite_quant_size)               # Quantized TFLite model\n",
        "print(\"Quantized model with QA in Mb:\", TFLite_quant_QA_size )   # Quantized TFLite model with Quantization Awareness"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe that the quantized TFLite models are 4x smaller!"
      ],
      "metadata": {
        "id": "hVCUG_V0qer2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization of TFLite model Test Accuracies and Sizes"
      ],
      "metadata": {
        "id": "HWSIWc9r_koH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "accuracies = {'Baseline model':TFLite_baseline_accuracy, 'Quantized model':TFLite_quant_accuracy, 'Quantized model with QA':TFLite_quant_QA_accuracy}\n",
        "models = list(accuracies.keys())\n",
        "acc = list(accuracies.values())\n",
        "\n",
        "fig = plt.figure(figsize = (10, 5))\n",
        "plt.ylim(0.95, 0.965)\n",
        "\n",
        "# creating the bar plot\n",
        "plt.bar(models, acc, color ='maroon',\n",
        "        width = 0.4)\n",
        "\n",
        "plt.ylabel(\"Test Accuracies\")\n",
        "plt.title(\"TFLite Model Performance on MNIST Test Dataset\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gKVEqsd_J6xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sizes = {'Baseline model':TFLite_baseline_size, 'Quantized model':TFLite_quant_size, 'Quantized model with QA':TFLite_quant_QA_size}\n",
        "models = list(sizes.keys())\n",
        "size = list(sizes.values())\n",
        "\n",
        "fig = plt.figure(figsize = (10, 5))\n",
        "\n",
        "# creating the bar plot\n",
        "plt.bar(models, size, color ='blue',\n",
        "        width = 0.4)\n",
        "\n",
        "plt.ylabel(\"Size in MB\")\n",
        "plt.title(\"TFLite Model Sizes\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hsTEhJ9_Low0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O5xuci-SonI"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2I7xmyMW5QY"
      },
      "source": [
        "In this tutorial, you saw how to create quantization aware models with the TensorFlow Model Optimization Toolkit API and then quantized models for the TFLite backend.You saw a 4x model size compression benefit for a model for MNIST, with minimal accuracy difference.\n",
        "\n",
        "To see the latency benefits on mobile, try out the TFLite examples [in the TFLite app repository](https://www.tensorflow.org/lite/models).\n"
      ]
    }
  ]
}