{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "Before starting, you must click on the \"Copy To Drive\" option in the top bar. Go to File --> Save a Copy to Drive. Name it *'LastName_FirstName_T2.4.ipynb'*. <ins>This is the master notebook so you will not be able to save your changes without copying it !</ins> Once you click on that, make sure you are working on that version of the notebook so that your work is saved.\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "OSn_9mrHp8fG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In this tutorial, weâ€™ll understand the audio preprocessing pipeline. In the first section we will explore different ways to represent/visualize audio signals. In the second section, we will define our KWS model and run inference on the speech commands dataset. Finally, in the last section of the tutorial, we will test the model on a few audio samples!"
      ],
      "metadata": {
        "id": "O2PT-M0BQ5_0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9GCyPWNuOm7"
      },
      "source": [
        "# 1. Spectrograms and MFCCs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upgCc3gXybsA"
      },
      "source": [
        "### Install required Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We install the following packages -\n",
        "\n",
        "\n",
        "1.   [ffmpeg-python](https://pypi.org/project/ffmpeg-python/) - Python binding for ffmpeg (a free and open-source multimedia processing project). You can read more about it [here](https://ffmpeg.org/about.html).\n",
        "2.   [tensorflow-io](https://www.tensorflow.org/io) - provides additional Dataset, streaming, and file system extensions.\n",
        "3. [python_speech_features](https://github.com/jameslyons/python_speech_features) - provides common speech features including MFCCs.\n",
        "\n"
      ],
      "metadata": {
        "id": "wI0Bxk_SDxm0"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUDYyMZRfkX4"
      },
      "source": [
        "!pip install ffmpeg-python &> 0       # &> shell command to hide output\n",
        "!pip install tensorflow-io &> 0\n",
        "!pip install python_speech_features &> 0\n",
        "print(\"Packages Installed\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGzzi-siATEZ"
      },
      "source": [
        "### Import everything we will need"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   [IPython](https://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html) - Public API for display tools in IPython.\n",
        "2.    from [google.colab.output](https://colab.research.google.com/notebooks/snippets/advanced_outputs.ipynb#scrollTo=MprPsZJa3AQF) import eval_js - Evaluate a Javascript expression from Python\n",
        "3. [base64](https://docs.python.org/3/library/base64.html) - provides functions for encoding binary data to printable ASCII characters and decoding such encodings back to binary data.\n",
        "4. [scipy.io.wavfile](https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.wavfile.read.html) - Open a WAV file and returns the sample rate and data.\n",
        "5. [pickle](https://docs.python.org/3/library/pickle.html) - module for serializing and de-serializing a Python object structure\n",
        "6. [librosa](https://librosa.org/doc/latest/index.html) - python package for music and audio analysis\n",
        "\n"
      ],
      "metadata": {
        "id": "BU7NLAe4hZmE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV8869MMAZKa"
      },
      "source": [
        "from IPython.display import HTML, Audio\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "import tensorflow as tf\n",
        "import tensorflow_io as tfio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from python_speech_features import mfcc\n",
        "from matplotlib import cm\n",
        "import pickle\n",
        "import librosa\n",
        "print(\"Packages Imported\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJPb0BZuAcit"
      },
      "source": [
        "### Define the audio importing function.\n",
        "\n",
        "The cell below defines an audio recorder that allows us to record audio from our laptop's microphone.\n",
        "\n",
        "Adapted from:\n",
        "\n",
        "\n",
        "1.   https://ricardodeazambuja.com/deep_learning/2019/03/09/audio_and_video_google_colab/\n",
        "2.   https://colab.research.google.com/drive/1Z6VIRZ_sX314hyev3Gm5gBqvm1wQVo-a#scrollTo=RtMcXr3o6gxN\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xamv_S585KgI"
      },
      "source": [
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var my_div = document.createElement(\"DIV\");\n",
        "var my_p = document.createElement(\"P\");\n",
        "var my_btn = document.createElement(\"BUTTON\");\n",
        "var t = document.createTextNode(\"Press to start recording\");\n",
        "\n",
        "my_btn.appendChild(t);\n",
        "//my_p.appendChild(my_btn);\n",
        "my_div.appendChild(my_btn);\n",
        "document.body.appendChild(my_div);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "var recordButton = my_btn;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "  gumStream = stream;\n",
        "  var options = {\n",
        "    //bitsPerSecond: 8000, //chrome seems to ignore, always 48k\n",
        "    mimeType : 'audio/webm;codecs=opus'\n",
        "    //mimeType : 'audio/webm;codecs=pcm'\n",
        "  };\n",
        "  //recorder = new MediaRecorder(stream, options);\n",
        "  recorder = new MediaRecorder(stream);\n",
        "  recorder.ondataavailable = function(e) {\n",
        "    var url = URL.createObjectURL(e.data);\n",
        "    var preview = document.createElement('audio');\n",
        "    preview.controls = true;\n",
        "    preview.src = url;\n",
        "    document.body.appendChild(preview);\n",
        "\n",
        "    reader = new FileReader();\n",
        "    reader.readAsDataURL(e.data);\n",
        "    reader.onloadend = function() {\n",
        "      base64data = reader.result;\n",
        "      //console.log(\"Inside FileReader:\" + base64data);\n",
        "    }\n",
        "  };\n",
        "  recorder.start();\n",
        "  };\n",
        "\n",
        "recordButton.innerText = \"Recording... press to stop\";\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "\n",
        "function toggleRecording() {\n",
        "  if (recorder && recorder.state == \"recording\") {\n",
        "      recorder.stop();\n",
        "      gumStream.getAudioTracks()[0].stop();\n",
        "      recordButton.innerText = \"Saving the recording... pls wait!\"\n",
        "  }\n",
        "}\n",
        "\n",
        "// https://stackoverflow.com/a/951057\n",
        "function sleep(ms) {\n",
        "  return new Promise(resolve => setTimeout(resolve, ms));\n",
        "}\n",
        "\n",
        "var data = new Promise(resolve=>{\n",
        "//recordButton.addEventListener(\"click\", toggleRecording);\n",
        "recordButton.onclick = ()=>{\n",
        "toggleRecording()\n",
        "\n",
        "sleep(2000).then(() => {\n",
        "  // wait 2000ms for the data to be available...\n",
        "  // ideally this should use something like await...\n",
        "  //console.log(\"Inside data:\" + base64data)\n",
        "  resolve(base64data.toString())\n",
        "\n",
        "});\n",
        "\n",
        "}\n",
        "});\n",
        "\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "def get_audio():\n",
        "  display(HTML(AUDIO_HTML))\n",
        "  data = eval_js(\"data\")\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "\n",
        "  process = (ffmpeg\n",
        "    .input('pipe:0')\n",
        "    .output('pipe:1', format='wav')\n",
        "    .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)\n",
        "  )\n",
        "  output, err = process.communicate(input=binary)\n",
        "\n",
        "  riff_chunk_size = len(output) - 8\n",
        "  # Break up the chunk size into four bytes, held in b.\n",
        "  q = riff_chunk_size\n",
        "  b = []\n",
        "  for i in range(4):\n",
        "      q, r = divmod(q, 256)\n",
        "      b.append(r)\n",
        "\n",
        "  # Replace bytes 4:8 in proc.stdout with the actual size of the RIFF chunk.\n",
        "  riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "  sr, audio = wav_read(io.BytesIO(riff))      # returns sample rate, data from wav file\n",
        "\n",
        "  return audio, sr\n",
        "\n",
        "print(\"Chrome Audio Recorder Defined\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf \"/content/dataset\"\n",
        "# !rm -rf \"/content/extract_loudest_section\"\n",
        "# !rm -rf \"/content/logs\"\n",
        "# !rm -rf \"/content/models\"\n",
        "# !rm -rf \"/content/sample_data\"\n",
        "# !rm -rf \"/content/tensorflow\"\n",
        "# !rm -rf \"/content/train\"\n",
        "# !rm -rf \"/content/trimmed\"\n",
        "\n",
        "# !rm -rf \"/content/tensorflow-2.4.1\""
      ],
      "metadata": {
        "id": "1_UmuqB1PVfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AY-xYPiLAj5j"
      },
      "source": [
        "### Load in the Audio Samples\n",
        "###**[A] Record your own audio samples!**\n",
        "\n",
        "In the below cells, we can record our own audio samples for **'loud yes'**, **'quiet yes'**, **'loud no'**, and **'quiet no'**.\n",
        "\n",
        "After you run each cell wait for the stop button to appear then start recording and then press the button to stop the recording once you have said the word!\n",
        "\n",
        "**If you do not want to record audio then there is a way to load in the default audio used in the lecture slides.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-SSG2lv5Mg9"
      },
      "source": [
        "# audio_yes_loud, sr_yes_loud = get_audio()\n",
        "# print(\"DONE\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyB7qaDM6iQj"
      },
      "source": [
        "# audio_yes_quiet, sr_yes_quiet = get_audio()\n",
        "# print(\"DONE\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhtrUYWT6ti6"
      },
      "source": [
        "# audio_no_loud, sr_no_loud = get_audio()\n",
        "# print(\"DONE\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wO4wQp9D98Ds"
      },
      "source": [
        "# audio_no_quiet, sr_no_quiet = get_audio()\n",
        "# print(\"DONE\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "**Question 1**: Why are we recording both loud and soft versions of a word?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "YueVxHzyVC0Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving custom audio files\n",
        "\n",
        "If you would like to save your files for later, uncomment and run the lines of code in the below cell. you can find your files in the folder icon to the left (left tab)."
      ],
      "metadata": {
        "id": "nAt6hA0BSHtA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQiPGQWubCjp"
      },
      "source": [
        "# audio_files = {\n",
        "#   'audio_yes_loud': audio_yes_loud, 'sr_yes_loud': sr_yes_loud,\n",
        "#   'audio_yes_quiet': audio_yes_quiet, 'sr_yes_quiet': sr_yes_quiet,\n",
        "#   'audio_no_loud': audio_no_loud, 'sr_no_loud': sr_no_loud,\n",
        "#   'audio_no_quiet': audio_no_quiet, 'sr_no_quiet': sr_no_quiet,\n",
        "# }\n",
        "# with open('audio_files.pkl', 'wb') as fid:            ##  store python object\n",
        "#   pickle.dump(audio_files, fid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1mc3l3bZbJv"
      },
      "source": [
        "### **[B] Load in default audio samples**\n",
        "Instead of recording your own audio samples, you can load in some default ones. **Don't run this cell if you have recorded your own audio samples!**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We download a pickle file from github that contains the default audio samples using the [wget](https://www.gnu.org/software/wget/) command. **Wait for the file to sync in the Colab and then run the next cell!**"
      ],
      "metadata": {
        "id": "CMRE4uM3TMQn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vIjpKveaSzm"
      },
      "source": [
        "!wget --no-check-certificate --content-disposition https://github.com/tinyMLx/colabs/blob/master/audio_files.pkl?raw=true\n",
        "print(\"Wait a minute for the file to sync in the Colab and then run the next cell!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gb3WbdPPdCmu"
      },
      "source": [
        "fid = open('audio_files.pkl', 'rb')\n",
        "audio_files = pickle.load(fid)                          # python dictionary\n",
        "audio_yes_loud = audio_files['audio_yes_loud']\n",
        "sr_yes_loud = audio_files['sr_yes_loud']\n",
        "audio_yes_quiet = audio_files['audio_yes_quiet']\n",
        "sr_yes_quiet = audio_files['sr_yes_quiet']\n",
        "audio_no_loud = audio_files['audio_no_loud']\n",
        "sr_no_loud = audio_files['sr_no_loud']\n",
        "audio_no_quiet = audio_files['audio_no_quiet']\n",
        "sr_no_quiet = audio_files['sr_no_quiet']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inspect audio sample"
      ],
      "metadata": {
        "id": "3qaJIVOFT5pE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"audio_yes_loud:\", audio_yes_loud)\n",
        "print(\"type:\", type(audio_yes_loud))\n",
        "print(\"shape:\", audio_yes_loud.shape)"
      ],
      "metadata": {
        "id": "LMA2ME33Td0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FKjT0VHZLvD"
      },
      "source": [
        "### Listen to Loaded Audio Samples\n",
        "You can hear the audio files you just loaded below. IPython gives us a widget to play audio files through a notebook.\n",
        "\n",
        "***Note*** - the loud yes and no audio could be too loud!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toHWCy1JZgzH"
      },
      "source": [
        "Audio(audio_yes_loud, rate=sr_yes_loud)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUPW18JEZr33"
      },
      "source": [
        "Audio(audio_yes_quiet, rate=sr_yes_quiet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhsfxQk5ZsB_"
      },
      "source": [
        "Audio(audio_no_loud, rate=sr_no_loud)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPcNYzcaZsJy"
      },
      "source": [
        "Audio(audio_no_quiet, rate=sr_no_quiet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBE2WWudAo4D"
      },
      "source": [
        "### Visualize the samples\n",
        "[A] We will first visualize the audio samples as **signals**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "il58WctF5fPV"
      },
      "source": [
        "# Plot the figure (axes - 10e-5)\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2)\n",
        "max_val = max(np.append(np.append(np.append(audio_yes_loud,audio_yes_quiet),audio_no_loud),audio_no_quiet))   # for y-axis range\n",
        "ax1.plot(audio_yes_loud)\n",
        "ax1.set_title(\"Yes Loud\", {'fontsize':20, 'fontweight':'bold'})\n",
        "ax1.set_ylim(-max_val, max_val)\n",
        "ax2.plot(audio_yes_quiet)\n",
        "ax2.set_title(\"Yes Quiet\", {'fontsize':20, 'fontweight':'bold'})\n",
        "ax2.set_ylim(-max_val, max_val)\n",
        "ax3.plot(audio_no_loud)\n",
        "ax3.set_title(\"No Loud\", {'fontsize':20, 'fontweight':'bold'})\n",
        "ax3.set_ylim(-max_val, max_val)\n",
        "ax4.plot(audio_no_quiet)\n",
        "ax4.set_title(\"No Quiet\", {'fontsize':20, 'fontweight':'bold'})\n",
        "ax4.set_ylim(-max_val, max_val)\n",
        "fig.set_size_inches(18,12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61oIZujzLU4d"
      },
      "source": [
        "\n",
        "[B] Next, we will view the Fourier Transform of the signal i.e., the **signal in the frequency domain**. We will use `numpy.fft.fft()` for this. You can read more about the API [here](https://numpy.org/doc/stable/reference/generated/numpy.fft.fft.html).\n",
        "\n",
        "This implementation was adapted from -\n",
        "\n",
        "\n",
        "1.   https://makersportal.com/blog/2018/9/13/audio-processing-in-python-part-i-sampling-and-the-fast-fourier-transform\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gf18UcyhM571"
      },
      "source": [
        "# compute the FFT and take the single-sided spectrum only - computes magnitude of complex number\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2)\n",
        "\n",
        "ft_audio_yes_loud = np.abs(2*np.fft.fft(audio_yes_loud))        # numpy.abs() gives magnitude of a complex number\n",
        "ft_audio_yes_quiet = np.abs(2*np.fft.fft(audio_yes_quiet))\n",
        "ft_audio_no_loud = np.abs(2*np.fft.fft(audio_no_loud))\n",
        "ft_audio_no_quiet = np.abs(2*np.fft.fft(audio_no_quiet))\n",
        "\n",
        "# Plot the figure\n",
        "ax1.plot(ft_audio_yes_loud)\n",
        "ax1.set_xscale('log')\n",
        "ax1.set_yscale('log')\n",
        "ax1.set_title(\"Yes Loud\", {'fontsize':20, 'fontweight':'bold'})\n",
        "ax2.plot(ft_audio_yes_quiet)\n",
        "ax2.set_xscale('log')\n",
        "ax2.set_yscale('log')\n",
        "ax2.set_title(\"Yes Quiet\", {'fontsize':20, 'fontweight':'bold'})\n",
        "ax3.plot(ft_audio_no_loud)\n",
        "ax3.set_xscale('log')\n",
        "ax3.set_yscale('log')\n",
        "ax3.set_title(\"No Loud\", {'fontsize':20, 'fontweight':'bold'})\n",
        "ax4.plot(ft_audio_no_quiet)\n",
        "ax4.set_xscale('log')\n",
        "ax4.set_yscale('log')\n",
        "ax4.set_title(\"No Quiet\", {'fontsize':20, 'fontweight':'bold'})\n",
        "fig.set_size_inches(18,12)\n",
        "fig.text(0.5, 0.06, 'Frequency [Hz]', {'fontsize':20, 'fontweight':'bold'}, ha='center');\n",
        "fig.text(0.08, 0.5, 'Amplitude', {'fontsize':20, 'fontweight':'bold'}, va='center', rotation='vertical');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou4Jf5U1Avuo"
      },
      "source": [
        "[C] Next, we will visualize the audio samples as spectrograms. We will be using `tfio.audio.spectrogram()` for this. You can read more about the API [here](https://www.tensorflow.org/io/api_docs/python/tfio/audio/spectrogram).\n",
        "\n",
        "The implementation was adapted from -\n",
        "\n",
        "\n",
        "1.   https://aruno14.medium.com/comparaison-of-audio-representation-in-tensorflow-b6c33a83d77f\n",
        "\n",
        "\n",
        "\n",
        "Can you see how spectrograms can help machine learning models better differentiate between audio samples?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIOmPv975gYW"
      },
      "source": [
        "# Convert to spectrogram and display\n",
        "# adapted from https://aruno14.medium.com/comparaison-of-audio-representation-in-tensorflow-b6c33a83d77f\n",
        "spectrogram_yes_loud = tfio.audio.spectrogram(audio_yes_loud/1.0, nfft=2048, window=len(audio_yes_loud), stride=int(sr_yes_loud * 0.008))\n",
        "spectrogram_yes_quiet = tfio.audio.spectrogram(audio_yes_quiet/1.0, nfft=2048, window=len(audio_yes_quiet), stride=int(sr_yes_quiet * 0.008))\n",
        "spectrogram_no_loud = tfio.audio.spectrogram(audio_no_loud/1.0, nfft=2048, window=len(audio_no_loud), stride=int(sr_no_loud * 0.008))\n",
        "spectrogram_no_quiet = tfio.audio.spectrogram(audio_no_quiet/1.0, nfft=2048, window=len(audio_no_quiet), stride=int(sr_no_quiet * 0.008))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# spectrogram 1\n",
        "spectrogram_yes_loud = tf.math.log(spectrogram_yes_loud)\n",
        "# spectrogram_yes_loud = spectrogram_yes_loud.eval(session=tf.compat.v1.Session())      # to work with TF 2.x\n",
        "spectrogram_yes_loud = spectrogram_yes_loud.numpy()\n",
        "\n",
        "# spectrogram 2\n",
        "spectrogram_yes_quiet = tf.math.log(spectrogram_yes_quiet)\n",
        "# spectrogram_yes_quiet = spectrogram_yes_quiet.eval(session=tf.compat.v1.Session())\n",
        "spectrogram_yes_quiet = spectrogram_yes_quiet.numpy()\n",
        "\n",
        "# spectrogram 3\n",
        "spectrogram_no_loud = tf.math.log(spectrogram_no_loud)\n",
        "# spectrogram_no_loud = spectrogram_no_loud.eval(session=tf.compat.v1.Session())\n",
        "spectrogram_no_loud = spectrogram_no_loud.numpy()\n",
        "\n",
        "# spectrogram 4\n",
        "spectrogram_no_quiet = tf.math.log(spectrogram_no_quiet)\n",
        "# spectrogram_no_quiet = spectrogram_no_quiet.eval(session=tf.compat.v1.Session())\n",
        "spectrogram_no_quiet = spectrogram_no_quiet.numpy()\n",
        "\n",
        "\n",
        "# Plot the figure\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2)\n",
        "ax1.imshow(spectrogram_yes_loud, aspect='auto')\n",
        "ax1.set_title(\"Yes Loud\", {'fontsize':20, 'fontweight':'bold'})\n",
        "ax2.imshow(spectrogram_yes_quiet, aspect='auto')\n",
        "ax2.set_title(\"Yes Quiet\", {'fontsize':20, 'fontweight':'bold'})\n",
        "ax3.imshow(spectrogram_no_loud, aspect='auto')\n",
        "ax3.set_title(\"No Loud\", {'fontsize':20, 'fontweight':'bold'})\n",
        "ax4.imshow(spectrogram_no_quiet, aspect='auto')\n",
        "ax4.set_title(\"No Quiet\", {'fontsize':20, 'fontweight':'bold'})\n",
        "fig.set_size_inches(18,12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thaEF4eV0Wol"
      },
      "source": [
        "[D] Finally, we will visualize the audio samples as **MFCCs** thereby using the Mel Scale to better associate the features to human hearing! We use the librosa library to achieve this. You can read more about the APIs we use here - [Link1](https://librosa.org/doc/main/generated/librosa.feature.melspectrogram.html), [Link2](https://librosa.org/doc/main/generated/librosa.power_to_db.html). This implementation was adapted from -\n",
        "\n",
        "\n",
        "1.   https://towardsdatascience.com/getting-to-know-the-mel-spectrogram-31bca3e2d9d0\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qh-D5tV20VwG"
      },
      "source": [
        "# Convert to MFCC using the Mel Scale\n",
        "mfcc_yes_loud = librosa.power_to_db(librosa.feature.melspectrogram(\n",
        "    np.float32(audio_yes_loud), sr=sr_yes_loud, n_fft=2048, hop_length=512, n_mels=128), ref=np.max)\n",
        "mfcc_yes_quiet = librosa.power_to_db(librosa.feature.melspectrogram(\n",
        "    np.float32(audio_yes_quiet), sr=sr_yes_quiet, n_fft=2048, hop_length=512, n_mels=128), ref=np.max)\n",
        "mfcc_no_loud = librosa.power_to_db(librosa.feature.melspectrogram(\n",
        "    np.float32(audio_no_loud), sr=sr_no_loud, n_fft=2048, hop_length=512, n_mels=128), ref=np.max)\n",
        "mfcc_no_quiet = librosa.power_to_db(librosa.feature.melspectrogram(\n",
        "    np.float32(audio_no_quiet), sr=sr_no_quiet, n_fft=2048, hop_length=512, n_mels=128), ref=np.max)\n",
        "\n",
        "# Plot the figure\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2)\n",
        "ax1.imshow(np.swapaxes(mfcc_yes_loud, 0 ,1), interpolation='nearest', cmap=cm.viridis, origin='lower', aspect='auto')\n",
        "ax1.set_title(\"Yes Loud\", {'fontsize':20, 'fontweight':'bold'})\n",
        "ax1.set_ylim(ax1.get_ylim()[::-1])\n",
        "ax2.imshow(np.swapaxes(mfcc_yes_quiet, 0 ,1), interpolation='nearest', cmap=cm.viridis, origin='lower', aspect='auto')\n",
        "ax2.set_title(\"Yes Quiet\", {'fontsize':20, 'fontweight':'bold'})\n",
        "ax2.set_ylim(ax2.get_ylim()[::-1])\n",
        "ax3.imshow(np.swapaxes(mfcc_no_loud, 0 ,1), interpolation='nearest', cmap=cm.viridis, origin='lower', aspect='auto')\n",
        "ax3.set_title(\"No Loud\", {'fontsize':20, 'fontweight':'bold'})\n",
        "ax3.set_ylim(ax3.get_ylim()[::-1])\n",
        "ax4.imshow(np.swapaxes(mfcc_no_quiet, 0 ,1), interpolation='nearest', cmap=cm.viridis, origin='lower', aspect='auto')\n",
        "ax4.set_title(\"No Quiet\", {'fontsize':20, 'fontweight':'bold'})\n",
        "ax4.set_ylim(ax4.get_ylim()[::-1])\n",
        "fig.set_size_inches(18,12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Keyword Spotting Model\n",
        "In this section we will see how well a default pre-trained model works for the Keyword Spotting application.\n",
        "\n",
        "This notebook uses a pre-trained 20 kB model based on [Simple Audio Recognition](https://www.tensorflow.org/tutorials/audio/simple_audio) to recognize keywords! The model is derived from a [micro_speech](https://github.com/tensorflow/tensorflow/tree/v2.4.1/tensorflow/lite/micro/examples/micro_speech) example for [TensorFlow Lite for MicroControllers](https://www.tensorflow.org/lite/microcontrollers/overview)"
      ],
      "metadata": {
        "id": "AMAgVtuQqcXi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries\n",
        "We import the following libraries.\n",
        "\n"
      ],
      "metadata": {
        "id": "3mx44pgsrmoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "!wget https://github.com/tensorflow/tensorflow/archive/v2.4.1.zip\n",
        "!unzip v2.4.1.zip &> 0\n",
        "!mv tensorflow-2.4.1/ tensorflow/          # git repo"
      ],
      "metadata": {
        "id": "lgeMvaTJrH-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "# We add this path so we can import the speech processing modules.\n",
        "sys.path.append(\"/content/tensorflow/tensorflow/examples/speech_commands\")\n",
        "\n",
        "import input_data\n",
        "import models\n",
        "import numpy as np\n",
        "import pickle\n"
      ],
      "metadata": {
        "id": "nU7rnUZeBBqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We clone the TensorFlow Github Repository, which contains the relevant code required to run this tutorial. The repository can be found on the left tab under /tensorflow."
      ],
      "metadata": {
        "id": "vAxaYbJVib_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure Defaults\n",
        "In the below cell, we define the words we want to train our model on. We define a comma-delimited list of the words you want to train for. All the other words you do not select will be used to train an \"unknown\" label so that the model does not just recognize speech but your specific words. Audio data with no spoken words will be used to train a \"silence\" label."
      ],
      "metadata": {
        "id": "VG9CyFTlrqds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "WANTED_WORDS = \"yes,no\"\n",
        "\n",
        "# Print the configuration to confirm it\n",
        "print(\"Spotting these words: %s\" % WANTED_WORDS)"
      ],
      "metadata": {
        "id": "ywzfuLefrwje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the default configurations to use the pre-trained model. **DO NOT MODIFY** the following constants as they include filepaths used in this notebook and data that is shared during training and inference."
      ],
      "metadata": {
        "id": "kHTmMeFJrzoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the percentage of 'silence' and 'unknown' training samples required\n",
        "# to ensure that we have equal number of samples for each label.\n",
        "\n",
        "number_of_labels = WANTED_WORDS.count(',') + 1                               # count() counts the number of commas (substr provided)\n",
        "number_of_total_labels = number_of_labels + 2                                # for 'silence' and 'unknown' label\n",
        "equal_percentage_of_training_samples = int(100.0/(number_of_total_labels))\n",
        "SILENT_PERCENTAGE = equal_percentage_of_training_samples\n",
        "UNKNOWN_PERCENTAGE = equal_percentage_of_training_samples\n",
        "\n",
        "\n",
        "# Constants which are shared during training and inference\n",
        "PREPROCESS = 'micro'\n",
        "WINDOW_STRIDE = 20\n",
        "MODEL_ARCHITECTURE = 'tiny_conv'\n",
        "\n",
        "\n",
        "# Constants for training directories and filepaths\n",
        "DATASET_DIR =  'dataset/'\n",
        "LOGS_DIR = 'logs/'\n",
        "TRAIN_DIR = 'train/' # for training checkpoints and other files.\n",
        "\n",
        "\n",
        "# Constants for inference directories and filepaths\n",
        "import os\n",
        "MODELS_DIR = 'models'\n",
        "if not os.path.exists(MODELS_DIR):\n",
        "  os.mkdir(MODELS_DIR)\n",
        "MODEL_TF = os.path.join(MODELS_DIR, 'model.pb')\n",
        "MODEL_TFLITE = os.path.join(MODELS_DIR, 'model.tflite')\n",
        "FLOAT_MODEL_TFLITE = os.path.join(MODELS_DIR, 'float_model.tflite')\n",
        "MODEL_TFLITE_MICRO = os.path.join(MODELS_DIR, 'model.cc')\n",
        "SAVED_MODEL = os.path.join(MODELS_DIR, 'saved_model')\n",
        "\n",
        "\n",
        "# Constants for Quantization\n",
        "QUANT_INPUT_MIN = 0.0\n",
        "QUANT_INPUT_MAX = 26.0\n",
        "QUANT_INPUT_RANGE = QUANT_INPUT_MAX - QUANT_INPUT_MIN\n",
        "\n",
        "\n",
        "# Constants for audio process during Quantization and Evaluation\n",
        "SAMPLE_RATE = 16000\n",
        "CLIP_DURATION_MS = 1000\n",
        "WINDOW_SIZE_MS = 30.0\n",
        "FEATURE_BIN_COUNT = 40\n",
        "BACKGROUND_FREQUENCY = 0.8\n",
        "BACKGROUND_VOLUME_RANGE = 0.1\n",
        "TIME_SHIFT_MS = 100.0\n",
        "\n",
        "\n",
        "# URL for the dataset and train/val/test split\n",
        "DATA_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz'\n",
        "VALIDATION_PERCENTAGE = 10\n",
        "TESTING_PERCENTAGE = 10"
      ],
      "metadata": {
        "id": "X01G-i-Gr0Qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the pre-trained model\n",
        "\n",
        "These commands will download a pre-trained model checkpoint file (the output from training) that we can use to build a model. You can read more about saving model checkpoints and using it for subsequent model inference here - [Link1](https://www.tensorflow.org/tutorials/keras/save_and_load), [Link2](https://www.tensorflow.org/guide/checkpoint)\n",
        "\n",
        "You should see a train/ folder on the left tab. It contains the checkpoint and metadata.  "
      ],
      "metadata": {
        "id": "q0ybpFwBr6S9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O \"https://storage.googleapis.com/download.tensorflow.org/models/tflite/speech_micro_train_2020_05_10.tgz\"\n",
        "!tar xzf speech_micro_train_2020_05_10.tgz\n",
        "TOTAL_STEPS = 15000  # used to identify which checkpoint file"
      ],
      "metadata": {
        "id": "F6PeZ2-Wr9lX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate a TensorFlow Model for Inference\n",
        "\n",
        "Below, we combine relevant training results (graph, weights, etc) into a single file for inference. This process is known as **freezing** a model and the resulting model is known as a frozen model/graph, as it cannot be further re-trained after this process.\n",
        "\n",
        "We run the freeze.py script to achieve this.\n",
        "\n",
        "After running this cell, you should find the saved model under models/ in the left tab."
      ],
      "metadata": {
        "id": "pvd7w59ssAdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf {SAVED_MODEL}\n",
        "\n",
        "!python tensorflow/tensorflow/examples/speech_commands/freeze.py \\\n",
        "--wanted_words=$WANTED_WORDS \\\n",
        "--window_stride_ms=$WINDOW_STRIDE \\\n",
        "--preprocess=$PREPROCESS \\\n",
        "--model_architecture=$MODEL_ARCHITECTURE \\\n",
        "--start_checkpoint=$TRAIN_DIR$MODEL_ARCHITECTURE'.ckpt-'{TOTAL_STEPS} \\\n",
        "--save_format=saved_model \\\n",
        "--output_file={SAVED_MODEL}"
      ],
      "metadata": {
        "id": "VIyfKFajsCfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate a TensorFlow Lite Model\n",
        "\n",
        "We convert the frozen graph into a TensorFlow Lite model, which is fully quantized for use with embedded devices. The following cell will also print the model size, which should be under 20 kilobytes.\n",
        "\n"
      ],
      "metadata": {
        "id": "yVCKepeUsHCi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we download the dataset to use as a representative dataset for more thoughtful post training quantization. We define an Audio Processor based on defined model settings that handles loading, partitioning, and preparing audio training data.\n",
        "\n",
        "To read more about the `models.prepare_model_settings()` method and `input_data.AudioProcessor()` method you can read the implementation in src files under path \"/content/tensorflow/tensorflow/examples/speech_commands/models.py\" and \"/content/tensorflow/tensorflow/examples/speech_commands/input_data.py\". These two python files can be found in the tab to the left.  \n",
        "\n",
        "**Note: this cell may take some time to run as it is a relatively large file** (Took around ~15 minutes)"
      ],
      "metadata": {
        "id": "-2uHvr_HrTeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_settings = models.prepare_model_settings(\n",
        "    len(input_data.prepare_words_list(WANTED_WORDS.split(','))),\n",
        "    SAMPLE_RATE, CLIP_DURATION_MS, WINDOW_SIZE_MS,\n",
        "    WINDOW_STRIDE, FEATURE_BIN_COUNT, PREPROCESS)\n",
        "\n",
        "audio_processor = input_data.AudioProcessor(\n",
        "    DATA_URL, DATASET_DIR,\n",
        "    SILENT_PERCENTAGE, UNKNOWN_PERCENTAGE,\n",
        "    WANTED_WORDS.split(','), VALIDATION_PERCENTAGE,\n",
        "    TESTING_PERCENTAGE, model_settings, LOGS_DIR)"
      ],
      "metadata": {
        "id": "nQaLWbMVsJG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create float and quantized TFLite models below. We use Integer Quantization."
      ],
      "metadata": {
        "id": "ZXaBzZ4Aso8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#with tf.Session() as sess:                                           #replaces the below line for use with TF1.x\n",
        "with tf.compat.v1.Session() as sess:\n",
        "\n",
        "  # float model\n",
        "  float_converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL)\n",
        "  float_tflite_model = float_converter.convert()\n",
        "  float_tflite_model_size = open(FLOAT_MODEL_TFLITE, \"wb\").write(float_tflite_model)\n",
        "  print()\n",
        "  print(\"Float model is %d bytes\" % float_tflite_model_size)\n",
        "\n",
        "  # quantized model\n",
        "  converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL)\n",
        "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "  #converter.inference_input_type = tf.lite.constants.INT8            #replaces the below line for use with TF1.x\n",
        "  converter.inference_input_type = tf.compat.v1.lite.constants.INT8\n",
        "  #converter.inference_output_type = tf.lite.constants.INT8           #replaces the below line for use with TF1.x\n",
        "  converter.inference_output_type = tf.compat.v1.lite.constants.INT8\n",
        "\n",
        "  def representative_dataset_gen():\n",
        "    for i in range(100):\n",
        "      data, _ = audio_processor.get_data(1, i*1, model_settings,\n",
        "                                         BACKGROUND_FREQUENCY,\n",
        "                                         BACKGROUND_VOLUME_RANGE,\n",
        "                                         TIME_SHIFT_MS,\n",
        "                                         'testing',\n",
        "                                         sess)\n",
        "      flattened_data = np.array(data.flatten(), dtype=np.float32).reshape(1, 1960)\n",
        "      yield [flattened_data]\n",
        "\n",
        "  converter.representative_dataset = representative_dataset_gen\n",
        "  tflite_model = converter.convert()\n",
        "  tflite_model_size = open(MODEL_TFLITE, \"wb\").write(tflite_model)\n",
        "  print(\"Quantized model is %d bytes\" % tflite_model_size)\n"
      ],
      "metadata": {
        "id": "QuOBeUrUsL8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "**Question 2**: What does audio_processor.get_data() do? Refer to input_data.py\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "XwuN0pQOsQ9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the accuracy after Quantization\n",
        "\n",
        "In the below cell, we run inference of our model on the entire test dataset."
      ],
      "metadata": {
        "id": "1EUeTmMBsOa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to run inference\n",
        "def run_tflite_inference_testSet(tflite_model_path, model_type=\"Float\"):\n",
        "\n",
        "  # extract test data\n",
        "  np.random.seed(0) # set random seed for reproducible test results.\n",
        "  #with tf.Session() as sess:                                                 #replaces the below line for use with TF1.x\n",
        "  with tf.compat.v1.Session() as sess:\n",
        "    test_data, test_labels = audio_processor.get_data(\n",
        "        -1, 0, model_settings, BACKGROUND_FREQUENCY, BACKGROUND_VOLUME_RANGE,\n",
        "        TIME_SHIFT_MS, 'testing', sess)\n",
        "  test_data = np.expand_dims(test_data, axis=1).astype(np.float32)\n",
        "\n",
        "\n",
        "\n",
        "  # Initialize the interpreter\n",
        "  interpreter = tf.lite.Interpreter(tflite_model_path)\n",
        "  interpreter.allocate_tensors()\n",
        "  input_details = interpreter.get_input_details()[0]\n",
        "  output_details = interpreter.get_output_details()[0]\n",
        "\n",
        "\n",
        "  # For quantized models, manually quantize the input data from float to integer\n",
        "  if model_type == \"Quantized\":\n",
        "    input_scale, input_zero_point = input_details[\"quantization\"]\n",
        "    test_data = test_data / input_scale + input_zero_point\n",
        "    test_data = test_data.astype(input_details[\"dtype\"])\n",
        "\n",
        "\n",
        "  # Evaluate the predictions\n",
        "  correct_predictions = 0\n",
        "  for i in range(len(test_data)):\n",
        "    interpreter.set_tensor(input_details[\"index\"], test_data[i])\n",
        "    interpreter.invoke()\n",
        "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
        "    top_prediction = output.argmax()\n",
        "    correct_predictions += (top_prediction == test_labels[i])\n",
        "\n",
        "\n",
        "  print('%s model accuracy is %f%% (Number of test samples=%d)' % (\n",
        "      model_type, (correct_predictions * 100) / len(test_data), len(test_data)))"
      ],
      "metadata": {
        "id": "zC5ex6jLsSXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute float model accuracy\n",
        "run_tflite_inference_testSet(FLOAT_MODEL_TFLITE)\n",
        "\n",
        "# Compute quantized model accuracy\n",
        "run_tflite_inference_testSet(MODEL_TFLITE, model_type='Quantized')"
      ],
      "metadata": {
        "id": "KUYNxURnsVRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Testing the model on example Audio\n",
        "Now that we know the model is fairly accurate on the test set lets explore with some hand crafted examples just how accurate the model is in the real world!"
      ],
      "metadata": {
        "id": "xagF3NxvsYLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load example audio samples\n",
        "\n",
        "Here, we download and load the 'yes_no.pkl' file that contains a few example audio samples for 'yes' and 'no'."
      ],
      "metadata": {
        "id": "eByMPnqKsdMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, Audio\n",
        "!wget --no-check-certificate --content-disposition https://github.com/tinyMLx/colabs/blob/master/yes_no.pkl?raw=true\n",
        "print(\"Wait a minute for the file to sync in the Colab and then run the next cell!\")"
      ],
      "metadata": {
        "id": "NE71yBzashe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fid = open('yes_no.pkl', 'rb')\n",
        "audio_files = pickle.load(fid)\n",
        "yes1 = audio_files['yes1']\n",
        "yes2 = audio_files['yes2']\n",
        "yes3 = audio_files['yes3']\n",
        "yes4 = audio_files['yes4']\n",
        "no1 = audio_files['no1']\n",
        "no2 = audio_files['no2']\n",
        "no3 = audio_files['no3']\n",
        "no4 = audio_files['no4']\n",
        "sr_yes1 = audio_files['sr_yes1']\n",
        "sr_yes2 = audio_files['sr_yes2']\n",
        "sr_yes3 = audio_files['sr_yes3']\n",
        "sr_yes4 = audio_files['sr_yes4']\n",
        "sr_no1 = audio_files['sr_no1']\n",
        "sr_no2 = audio_files['sr_no2']\n",
        "sr_no3 = audio_files['sr_no3']\n",
        "sr_no4 = audio_files['sr_no4']"
      ],
      "metadata": {
        "id": "uNC1JGVcslQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Audio(yes1, rate=sr_yes1)"
      ],
      "metadata": {
        "id": "qbaoFI47soE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Audio(yes2, rate=sr_yes2)"
      ],
      "metadata": {
        "id": "2U3dzRGzsqv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Audio(yes3, rate=sr_yes3)"
      ],
      "metadata": {
        "id": "WsDmlq0tss8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Audio(yes4, rate=sr_yes4)"
      ],
      "metadata": {
        "id": "5BIp_vSDsvRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Audio(no1, rate=sr_no1)"
      ],
      "metadata": {
        "id": "ZhFnxTE7sxWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Audio(no2, rate=sr_no2)"
      ],
      "metadata": {
        "id": "yS-27SX2szef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Audio(no3, rate=sr_no3)"
      ],
      "metadata": {
        "id": "jxy1O3W4s1lY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Audio(no4, rate=sr_no4)"
      ],
      "metadata": {
        "id": "yJ9MZLO1s4O-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the model on the Example Files\n",
        "We first need to import a series of packages and build the loudest section tool so that we can process audio files manually to send them to our model. These packages will also be used later for you to record your own audio to test the model!"
      ],
      "metadata": {
        "id": "ucEbcOANs7uy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ffmpeg-python &> 0\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "!pip install librosa\n",
        "import librosa\n",
        "import scipy.io.wavfile"
      ],
      "metadata": {
        "id": "X9pOIihXs-2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/petewarden/extract_loudest_section.git\n",
        "!make -C extract_loudest_section/\n",
        "print(\"Packages Imported, Extract_Loudest_Section Built\")"
      ],
      "metadata": {
        "id": "4bPc9F2tSQc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to run inference (on a single input this time)\n",
        "# Note: this also includes additional manual pre-processing\n",
        "\n",
        "TF_SESS = tf.compat.v1.InteractiveSession()\n",
        "def run_tflite_inference_singleFile(tflite_model_path, custom_audio, sr_custom_audio, model_type=\"Float\"):\n",
        "\n",
        "  # Preprocess the sample to get the features we pass to the model\n",
        "  # First re-sample to the needed rate (and convert to mono if needed)\n",
        "  custom_audio_resampled = librosa.resample(librosa.to_mono(np.float64(custom_audio)), sr_custom_audio, SAMPLE_RATE)\n",
        "  # Then extract the loudest one second\n",
        "  scipy.io.wavfile.write('custom_audio.wav', SAMPLE_RATE, np.int16(custom_audio_resampled))\n",
        "  !/tmp/extract_loudest_section/gen/bin/extract_loudest_section custom_audio.wav ./trimmed\n",
        "  # Finally pass it through the TFLiteMicro preprocessor to produce the\n",
        "  # spectrogram/MFCC input that the model expects\n",
        "  custom_model_settings = models.prepare_model_settings(\n",
        "      0, SAMPLE_RATE, CLIP_DURATION_MS, WINDOW_SIZE_MS,\n",
        "      WINDOW_STRIDE, FEATURE_BIN_COUNT, PREPROCESS)\n",
        "  custom_audio_processor = input_data.AudioProcessor(None, None, 0, 0, '', 0, 0,\n",
        "                                                    model_settings, None)\n",
        "  custom_audio_preprocessed = custom_audio_processor.get_features_for_wav(\n",
        "                                        'trimmed/custom_audio.wav', model_settings, TF_SESS)\n",
        "\n",
        "  # Reshape the output into a 1,1960 matrix as that is what the model expects\n",
        "  custom_audio_input = custom_audio_preprocessed[0].flatten()\n",
        "  test_data = np.reshape(custom_audio_input,(1,len(custom_audio_input)))\n",
        "\n",
        "\n",
        "  # Initialize the interpreter\n",
        "  interpreter = tf.lite.Interpreter(tflite_model_path)\n",
        "  interpreter.allocate_tensors()\n",
        "  input_details = interpreter.get_input_details()[0]\n",
        "  output_details = interpreter.get_output_details()[0]\n",
        "\n",
        "\n",
        "  # For quantized models, manually quantize the input data from float to integer\n",
        "  if model_type == \"Quantized\":\n",
        "    input_scale, input_zero_point = input_details[\"quantization\"]\n",
        "    test_data = test_data / input_scale + input_zero_point\n",
        "    test_data = test_data.astype(input_details[\"dtype\"])\n",
        "\n",
        "\n",
        "  # Run the interpreter\n",
        "  interpreter.set_tensor(input_details[\"index\"], test_data)\n",
        "  interpreter.invoke()\n",
        "  output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
        "  top_prediction = output.argmax()\n",
        "\n",
        "\n",
        "  # Translate the output\n",
        "  top_prediction_str = ''\n",
        "  if top_prediction == 2 or top_prediction == 3:\n",
        "    top_prediction_str = WANTED_WORDS.split(',')[top_prediction-2]\n",
        "  elif top_prediction == 0:\n",
        "    top_prediction_str = 'silence'\n",
        "  else:\n",
        "    top_prediction_str = 'unknown'\n",
        "\n",
        "  print('%s model guessed the value to be %s' % (model_type, top_prediction_str))"
      ],
      "metadata": {
        "id": "TUSNKGjdtCsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Then test the model -- do they all work as you'd expect?\n",
        "print(\"Testing yes1\")\n",
        "run_tflite_inference_singleFile(MODEL_TFLITE, yes1, sr_yes1, model_type=\"Quantized\")\n",
        "print(\"Testing yes2\")\n",
        "run_tflite_inference_singleFile(MODEL_TFLITE, yes2, sr_yes2, model_type=\"Quantized\")\n",
        "print(\"Testing yes3\")\n",
        "run_tflite_inference_singleFile(MODEL_TFLITE, yes3, sr_yes3, model_type=\"Quantized\")\n",
        "print(\"Testing yes4\")\n",
        "run_tflite_inference_singleFile(MODEL_TFLITE, yes4, sr_yes4, model_type=\"Quantized\")\n",
        "print(\"Testing no1\")\n",
        "run_tflite_inference_singleFile(MODEL_TFLITE, no1, sr_no1, model_type=\"Quantized\")\n",
        "print(\"Testing no2\")\n",
        "run_tflite_inference_singleFile(MODEL_TFLITE, no2, sr_no2, model_type=\"Quantized\")\n",
        "print(\"Testing no3\")\n",
        "run_tflite_inference_singleFile(MODEL_TFLITE, no3, sr_no3, model_type=\"Quantized\")\n",
        "print(\"Testing no4\")\n",
        "run_tflite_inference_singleFile(MODEL_TFLITE, no4, sr_no4, model_type=\"Quantized\")"
      ],
      "metadata": {
        "id": "EygKsOgqtFVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the model with your own data!\n",
        "\n",
        "Try recording your own audio to test the model. You can experiment with different ways to say 'yes' and 'no'. Also, test the 'unknown' and 'silence' classes."
      ],
      "metadata": {
        "id": "zTMgpayPtIy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the audio importing function\n",
        "Adapted from: https://ricardodeazambuja.com/deep_learning/2019/03/09/audio_and_video_google_colab/ and https://colab.research.google.com/drive/1Z6VIRZ_sX314hyev3Gm5gBqvm1wQVo-a#scrollTo=RtMcXr3o6gxN"
      ],
      "metadata": {
        "id": "nwu5auXStORa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var my_div = document.createElement(\"DIV\");\n",
        "var my_p = document.createElement(\"P\");\n",
        "var my_btn = document.createElement(\"BUTTON\");\n",
        "var t = document.createTextNode(\"Press to start recording\");\n",
        "\n",
        "my_btn.appendChild(t);\n",
        "my_div.appendChild(my_btn);\n",
        "document.body.appendChild(my_div);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "var recordButton = my_btn;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "  gumStream = stream;\n",
        "  var options = {\n",
        "    bitsPerSecond: 128000, //chrome seems to ignore, always 48k\n",
        "    audioBitsPerSecond: 128000, //chrome seems to ignore, always 48k\n",
        "    mimeType : 'audio/mp4'\n",
        "    // mimeType : 'audio/webm;codecs=opus' // try me if the above fails\n",
        "  };\n",
        "  recorder = new MediaRecorder(stream);\n",
        "  recorder.ondataavailable = function(e) {\n",
        "    var url = URL.createObjectURL(e.data);\n",
        "    var preview = document.createElement('audio');\n",
        "    preview.controls = true;\n",
        "    preview.src = url;\n",
        "    document.body.appendChild(preview);\n",
        "\n",
        "    reader = new FileReader();\n",
        "    reader.readAsDataURL(e.data);\n",
        "    reader.onloadend = function() {\n",
        "      base64data = reader.result;\n",
        "    }\n",
        "  };\n",
        "  recorder.start();\n",
        "  };\n",
        "\n",
        "recordButton.innerText = \"Recording... press to stop\";\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "\n",
        "function toggleRecording() {\n",
        "  if (recorder && recorder.state == \"recording\") {\n",
        "      recorder.stop();\n",
        "      gumStream.getAudioTracks()[0].stop();\n",
        "      recordButton.innerText = \"Saving the recording... pls wait!\"\n",
        "  }\n",
        "}\n",
        "\n",
        "// https://stackoverflow.com/a/951057\n",
        "function sleep(ms) {\n",
        "  return new Promise(resolve => setTimeout(resolve, ms));\n",
        "}\n",
        "\n",
        "var data = new Promise(resolve=>{\n",
        "//recordButton.addEventListener(\"click\", toggleRecording);\n",
        "recordButton.onclick = ()=>{\n",
        "toggleRecording()\n",
        "\n",
        "sleep(2000).then(() => {\n",
        "  // wait 2000ms for the data to be available...\n",
        "  // ideally this should use something like await...\n",
        "  resolve(base64data.toString())\n",
        "\n",
        "});\n",
        "\n",
        "}\n",
        "});\n",
        "\n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "def get_audio():\n",
        "  display(HTML(AUDIO_HTML))\n",
        "  data = eval_js(\"data\")\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "\n",
        "  process = (ffmpeg\n",
        "    .input('pipe:0')\n",
        "    .output('pipe:1', format='wav', ac='1')\n",
        "    .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)\n",
        "  )\n",
        "  output, err = process.communicate(input=binary)\n",
        "\n",
        "  riff_chunk_size = len(output) - 8\n",
        "  # Break up the chunk size into four bytes, held in b.\n",
        "  q = riff_chunk_size\n",
        "  b = []\n",
        "  for i in range(4):\n",
        "      q, r = divmod(q, 256)\n",
        "      b.append(r)\n",
        "\n",
        "  # Replace bytes 4:8 in proc.stdout with the actual size of the RIFF chunk.\n",
        "  riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "  sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "  return audio, sr\n",
        "print(\"Chrome Audio Recorder Defined\")"
      ],
      "metadata": {
        "id": "mZfED_wotMZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Record your own audio and test the model!\n",
        "After you run the record cell wait for the stop button to appear then start recording and then press the button to stop the recording once you have said the word!"
      ],
      "metadata": {
        "id": "B6eiVCxttWuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_audio, sr_custom_audio = get_audio()\n",
        "print(\"DONE\")"
      ],
      "metadata": {
        "id": "Nf14UxmbtZU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Then test the model\n",
        "run_tflite_inference_singleFile(MODEL_TFLITE, custom_audio, sr_custom_audio, model_type=\"Quantized\")"
      ],
      "metadata": {
        "id": "PH_seoDxtbt6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}