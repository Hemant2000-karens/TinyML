{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTD-AFtxqHgx"
   },
   "source": [
    "---\n",
    "\n",
    "\n",
    "Before starting, you must click on the \"Copy To Drive\" option in the top bar. Go to File --> Save a Copy to Drive. Name it *'LastName_FirstName_T2.5.ipynb'*. <ins>This is the master notebook so you will not be able to save your changes without copying it !</ins> Once you click on that, make sure you are working on that version of the notebook so that your work is saved.\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLt6Uw1x7uDl"
   },
   "source": [
    "### In this tutorial, we will explore a K-Means Clustering model to reliably distinguish between normal data and anomalous data. In the first section, we will see the performance of this model on a simple example. In the second section, we will evaluate the same model on more complex (higher dimensional) data and observe its drawbacks. Finally, in the last section, we will explore a Dimensionality Reduction technique to improve the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ifm3JBsFR8LT"
   },
   "source": [
    "# K-means Clustering for Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCRx0tPV7BZu"
   },
   "source": [
    "We will first import the required libraries:\n",
    "\n",
    "\n",
    "1.   [seaborn](https://seaborn.pydata.org/) - statistical data visualization in Python\n",
    "2.   [numpy](https://numpy.org/) - scientific computing in Python\n",
    "3.   [matplotlib](https://matplotlib.org/) - visualization of data in Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2f3xSlfzR8LT"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smmcSc4FR8LT"
   },
   "source": [
    "# 1. K-means for Anomaly Detection - Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqWluG89R8LT"
   },
   "source": [
    "K-means clustering is a simple and useful unsupervised learning algorithm. The goal of K-means clustering is to group similar data points into a set number (K) of groups. The algorithm does this by identifying 'centroids', which are the centers of clusters, and then allocating data points to the nearest cluster. You can read more about this machine learning technique [here](https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ek7cdug9Rz7b"
   },
   "source": [
    "### Generate Training Data\n",
    "Let's try a simple example where we generate clustered data and fit a K-Means model on the data. You could imagine these clusters as different stages of NORMAL operation of a machine.\n",
    "\n",
    "We import the following:\n",
    "\n",
    "\n",
    "1.    from sklearn.datasets import [make_blobs](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html) - Generates Gaussian blobs for clustering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kabhRNKxKNbX"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lb0SAaTmKSWc"
   },
   "source": [
    "In the below cell, we generate 300 data samples and assign them to 3 different clusters. We then visualize the data in 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yqxG6wjoR8LT"
   },
   "outputs": [],
   "source": [
    "num_centers = 3\n",
    "X_train, y_train_true = make_blobs(n_samples=300, centers=num_centers,\n",
    "                                   cluster_std=0.40, random_state=0)\n",
    "\n",
    "\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], s=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKzRVLELK-PV"
   },
   "source": [
    "### Observe Data and Ground Truth array shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0tXgzT4c42q3"
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train_true.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRfEjUUBR8LU"
   },
   "source": [
    "### Define K-Means model\n",
    "\n",
    "Now let's use [SKLearn's KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) algorithm to fit to the data. This does a lot of the work for us, but if you would like to learn more about the underlying process check out the [wikipedia page](https://en.wikipedia.org/wiki/K-means_clustering#Algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJIPmxAILsgm"
   },
   "source": [
    "We import the following:\n",
    "\n",
    "\n",
    "1.  [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) - free software machine learning library for Python.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8cIEPZgXL1GL"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZUJ0lCNL4SK"
   },
   "source": [
    "We define our K-Means model for a fixed number of clusters and then fit it to our X_train data samples. Notice, we don't fit the model on the ground truth values (y_true) - **Unsupervised Learning**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q4NIBZ6zR8LU"
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=num_centers)        # we select three clusters\n",
    "\n",
    "kmeans.fit(X_train)                            # we fit the centroids to the data\n",
    "y_train_kmeans = kmeans.predict(X_train)       # we determine the closest centroid for each datapoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_TQ8Oh8R8LU"
   },
   "source": [
    "Now let's visualize the results. Each datapoint is color-coded according to the centroid they correspond to, and the centroids themselves are shown as black circles.\n",
    "\n",
    "\n",
    "*Note:* The predictions displayed through color of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2pOtyuvhR8LU"
   },
   "outputs": [],
   "source": [
    "# plotting clusters\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train_kmeans, s=50, cmap='viridis')\n",
    "\n",
    "# centroid locations\n",
    "centers = kmeans.cluster_centers_\n",
    "print(centers)\n",
    "print()\n",
    "\n",
    "# plotting centroids\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ToMhXzc0R8LU"
   },
   "source": [
    "Looks like K-means does a great job in this simple example! Now let's explore how we can use this for anomaly detection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzwjhszlI3oD"
   },
   "source": [
    "---\n",
    "\n",
    "\n",
    "**Question 1**: Write down in layman's terms the steps involved in K-Means Clustering Algorithm.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ad3LqqKlNVpv"
   },
   "source": [
    "### K-Means for Anomaly Detection\n",
    "Below we introduce two new clusters (orange) that weren't part of our training data. We will pretend all of these are anomalies for the sake of a simple example.\n",
    "\n",
    "One of these clusters is completely different from the data we've seen before and another is only slightly different. We can easily (visually) separate one of the clusters, but the other one overlaps slightly with one of our training clusters. Given the low dimensionality of the data, it's reasonable that some new data is impossible to distinguish from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0nGf2_yOIVg"
   },
   "source": [
    "In the below cell, we generate 300 new data samples and assign them to 2 different clusters. We then visualize the anomalous data along with original training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZcyVY_nDX_Tn"
   },
   "outputs": [],
   "source": [
    "X_anomaly, y_anomaly_true = make_blobs(n_samples=300, centers=2,\n",
    "                       cluster_std=0.40, random_state=1)\n",
    "\n",
    "# plotting train dataset\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], s=50)\n",
    "\n",
    "# plotting anomalous dataset\n",
    "plt.scatter(X_anomaly[:,0], X_anomaly[:,1], s=50)\n",
    "\n",
    "plt.title(\"Original Training Data and Anomalous Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kh8TjeroaH1D"
   },
   "source": [
    "### Define a boundary to differentiate between Normal and Anomalous Data\n",
    "\n",
    "We will calculate the 99th percentile distance for each cluster's centroid which will act as a threshold to determine if a data sample is normal or anomalous.\n",
    "\n",
    "In the cells below, we will determine this threshold for each centroid using our training dataset (normal data).\n",
    "\n",
    "First we will calculate the distance between each cluster's centroid and its corresponding cluster's data samples. To do this, we first find the distance of all 300 points to each centroid. This is done using the `kmeans.transform()` API. You can read more about the API [here](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans.transform). We then assign the 300 points to a cluster (0, 1, or 2) based on its minimum distance to the centroid. For eg: if sample 40 has the following distances to the three centroids - [0.8, 0.6, 0.7], it will be assigned to cluster 1.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-SOS7eJaIO0"
   },
   "outputs": [],
   "source": [
    "# distances of each of the 300 training samples from each of the 3 centroids\n",
    "train_distances = kmeans.transform(X_train)\n",
    "print(\"train_distance shape:\", train_distances.shape)\n",
    "print()\n",
    "\n",
    "\n",
    "# distances of the 300 points from its closest centroid\n",
    "center_distances = {key: [] for key in range(num_centers)}      # {0: [distances...], 1: [distances...], 2: [distances...]}\n",
    "\n",
    "for i in range(len(y_train_kmeans)):  # 300\n",
    "  cluster_number = y_train_kmeans[i]\n",
    "  min_distance = train_distances[i][cluster_number]\n",
    "  #print(min_distance)\n",
    "\n",
    "  # min_distance2 = np.min(train_distances[i])\n",
    "  # print(min_distance2)\n",
    "\n",
    "  center_distances[cluster_number].append(min_distance)\n",
    "\n",
    "\n",
    "print(\"No. data samples close to centroid 1 (Cluster 1):\", len(center_distances[0]))\n",
    "print(\"No. data samples close to centroid 2 (Cluster 2):\", len(center_distances[1]))\n",
    "print(\"No. data samples close to centroid 3 (Cluster 3):\", len(center_distances[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3x7VH4RscatQ"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "We then calculate the 99 percentile distance for each centroid that was observed in the training data. We use the 99 percentile distance here since our training data could have some outliers.\n",
    "\n",
    "These distances will act as a boundary, beyond which we will classify datapoints as anomalies. The percentile can be adjusted to be more or less sensitive depending on the application and the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pi7d9Iu4YVy9"
   },
   "outputs": [],
   "source": [
    "percentile_threshold = 99\n",
    "\n",
    "\n",
    "# out of 100 data samples, it finds the 2nd max (or max among 99 samples) distance.\n",
    "center_99percentile_distance = dict()\n",
    "for key in center_distances.keys():      # 3\n",
    "    center_99percentile_distance[key] = np.percentile(center_distances[key], percentile_threshold)\n",
    "\n",
    "    # print(center_distances[key])\n",
    "    # print(np.max(center_distances[key]))\n",
    "    # print()\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"99 Percentile distance for each centroid:\",center_99percentile_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAXOnuPHl0b7"
   },
   "source": [
    "### Visualization of boundary\n",
    "Now let's plot those normal/abnomal boundaries on our training data to see how well they encompass our training data. We will also plot in yellow the points in our training data that are being classified as abnormal.\n",
    "\n",
    "To draw the circles, we use the `plt.Circle()` API. You can read more about the API [here](https://matplotlib.org/stable/api/_as_gen/matplotlib.patches.Circle.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yjpUYr3hibYV"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "colors = []\n",
    "for i in range(len(X_train)):\n",
    "  min_distance = train_distances[i][y_train_kmeans[i]]\n",
    "  cluster_num = y_train_kmeans[i]\n",
    "  if (min_distance > center_99percentile_distance[cluster_num]):\n",
    "    colors.append(3)\n",
    "  else:\n",
    "    colors.append(cluster_num)\n",
    "\n",
    "\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=colors, s=50, cmap='viridis')\n",
    "\n",
    "# draw circles\n",
    "for i in range(len(centers)):\n",
    "  circle = plt.Circle((centers[i][0], centers[i][1]), center_99percentile_distance[i], color='black', alpha=0.1);\n",
    "  ax.add_artist(circle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ainmt9SoHf-F"
   },
   "source": [
    "---\n",
    "\n",
    "\n",
    "**Question 2**: What is the role of the 99th percentile distance in the above plot (plt.Circle API)?\n",
    "\n",
    "**Answer:**  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9AR6TAUr3OH"
   },
   "source": [
    "### Classifying Test Data\n",
    "Now let's add in the abnormal test data to see how it's classified. We first get our predictions of the anomalous dataset using our trained model 'kmeans'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "19B6r5_OMFHU"
   },
   "outputs": [],
   "source": [
    "y_anomaly_kmeans = kmeans.predict(X_anomaly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhAspLmKMMGw"
   },
   "source": [
    "Then we combine the training data samples and testing data samples to visualize all samples on one plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T0qYhgT0r3hU"
   },
   "outputs": [],
   "source": [
    "# distances of each of the 300 testing samples from each of the 3 centroids\n",
    "anomaly_distances = kmeans.transform(X_anomaly)\n",
    "\n",
    "\n",
    "\n",
    "#combine all the data\n",
    "combined_distances = [*train_distances, *anomaly_distances]\n",
    "combined_y = [*y_train_kmeans, *y_anomaly_kmeans]\n",
    "all_data = np.array([*X_train, *X_anomaly])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0vb8QGZOAxf"
   },
   "source": [
    "We iterate through all data samples (normal and anomalous) and classify them based on their distance from the closest centroid. Here, we consider a data sample being classified as anomalous as positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O3_38bXCN-eE"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "false_neg=0\n",
    "false_pos=0\n",
    "\n",
    "colors = []\n",
    "for i in range(len(all_data)):\n",
    "  min_distance = combined_distances[i][combined_y[i]]\n",
    "  cluster_num = combined_y[i]\n",
    "\n",
    "  if (min_distance > center_99percentile_distance[cluster_num]):   # anomalous - positive\n",
    "    colors.append(3)  # any 4th color\n",
    "    if (i<300):       # training data is the first 300 elements in the combined list\n",
    "      false_pos+=1\n",
    "\n",
    "  else:               # normal - negative\n",
    "    colors.append(cluster_num)\n",
    "    if (i>=300):      # anomalous data is the last 300 elements in the combined list\n",
    "      false_neg+=1\n",
    "\n",
    "\n",
    "# cluster colors based on 'colors' array\n",
    "ax.scatter(all_data[:, 0], all_data[:, 1], c=colors, s=50, cmap='viridis')\n",
    "\n",
    "\n",
    "# plotting circles (boundaries)\n",
    "for i in range(len(centers)):\n",
    "  circle = plt.Circle((centers[i][0], centers[i][1]), center_99percentile_distance[i], color='black', alpha=0.1);\n",
    "  ax.add_artist(circle)\n",
    "\n",
    "\n",
    "\n",
    "print('Normal datapoints misclassified as abnormal: ', false_pos)             # abnormal assumed as positive\n",
    "print('Abnormal datapoints misclassified as normal: ', false_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1doZei2fLl8M"
   },
   "source": [
    "Our simple model did a pretty good job!\n",
    "\n",
    "Now we have a way to classify abnormal data in a simple two dimension space. You can adjust the `percentile_treshold` variable to see how that impacts the number of false positives and false negatives.\n",
    "\n",
    "Now let's see how well this applies to data with more dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHI-S7SmPmcB"
   },
   "source": [
    "---\n",
    "\n",
    "\n",
    "**Question 3**: Can you think of a scenario each for when a high false positve value and a high false negative value are bad?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDj3A9aCR8LV"
   },
   "source": [
    "# 2. K-means for Anomaly Detection on Digits - Example 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQE7b1CvOMt6"
   },
   "source": [
    "First we load in our dataset of **64** pixel images of numerical digits (think MNIST in 8x8 pixel images), **a much higher dimmension** than our 2-D problem we were dealing with earlier. The digits range from 0 - 9. We will treat digits 0 and 1 as abnormal and digits 2 - 9 as normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQX-KdxCO9VS"
   },
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KOIiHvXTR8LV"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "print(digits.data.shape)    # there are 1797 train examples - 64 features each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1pq_YbUQUw3"
   },
   "source": [
    "Since we consider 8 classes as normal (digits 2 - 9) we will assume 8 clusters for each normal label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zaQ-l8cnuF10"
   },
   "outputs": [],
   "source": [
    "normal_data = []\n",
    "abnormal_data = []\n",
    "\n",
    "normal_label = []\n",
    "abnormal_label = []\n",
    "\n",
    "num_clusters = 8\n",
    "\n",
    "#separate our data arbitrarily into normal (2-9) and abnormal (0-1)\n",
    "for i in range(len(digits.target)):\n",
    "  if digits.target[i] < 10-num_clusters:      # digits 0 and 1\n",
    "    abnormal_data.append(digits.data[i])\n",
    "    abnormal_label.append(digits.target[i])\n",
    "  else:                                       # digits 2 - 9\n",
    "    normal_data.append(digits.data[i])\n",
    "    normal_label.append(digits.target[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6A1yy0-PQe2S"
   },
   "source": [
    "### Inspect Data and Label Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h6ch_Vd_zYdl"
   },
   "outputs": [],
   "source": [
    "normal_data_np = np.array(normal_data)\n",
    "normal_label_np = np.array(normal_label)\n",
    "abnormal_data_np = np.array(abnormal_data)\n",
    "abnormal_label_np = np.array(abnormal_label)\n",
    "\n",
    "print(\"normal data shape:\", normal_data_np.shape)\n",
    "print(\"normal label shape:\", normal_label_np.shape)\n",
    "print(\"abnormal data shape:\", abnormal_data_np.shape)\n",
    "print(\"abnormal data shape:\", abnormal_label_np.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqQ1nb4qR8LV"
   },
   "source": [
    "### Define Model\n",
    "Next we find our 8 cluster centers/centroids (from normal data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WSXpxPoXR8LV"
   },
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
    "\n",
    "# train model on normal data\n",
    "kmeans.fit(normal_data)\n",
    "y_normal = kmeans.predict(normal_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlkqwnsyQ6uV"
   },
   "source": [
    "### Visualization of Centroids\n",
    "We observe below that the shape of each centroid is (1, 64) corresponding to the 64 pixels/features of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9v-DCDrZt7Xf"
   },
   "outputs": [],
   "source": [
    "print(kmeans.cluster_centers_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipRk2AV8R8LV"
   },
   "source": [
    "Next let's visualize the centers/centroids that K-means found for each cluster. We see that K-means is able to find clusters whose centers are recognizable digits. A bit blury, but still recognizable to the human eye!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "COPbqtpFR8LV"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, int(num_clusters/2), figsize=(8, 3))\n",
    "centers = kmeans.cluster_centers_.reshape(num_clusters, 8, 8)\n",
    "for axi, center in zip(ax.flat, centers):\n",
    "    axi.set(xticks=[], yticks=[])\n",
    "    axi.imshow(center, interpolation='nearest', cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6quDw3T4R8LV"
   },
   "source": [
    "### Define a boundary to differentiate between Normal and Anomalous Data\n",
    "First we will calculate the distance between each cluster's centroid and its corresponding cluster's data samples. To do this, we first find the distance of all 1437 normal data points (classes 2-9) to each centroid. This is done using the `kmeans.transform()` API. We then assign the 1437 points to a cluster (0 - 7) based on its minimum distance to the centroid. For eg: if sample 470 has the following distances to the eight centroids - [21.2, 21.4, 21.6, 21.8, **21.1**, 21.3, 21.5, 21.7], it will be assigned to cluster 4.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NVvtt7OUR8LV"
   },
   "outputs": [],
   "source": [
    "# distances of each of the 1437 normal training samples from each of the 8 centroids\n",
    "normal_distances =  # < YOUR CODE HERE >\n",
    "print(normal_distances.shape)\n",
    "\n",
    "\n",
    "center_distances = {key: [] for key in range(num_clusters)}\n",
    "for i in range(len(y_normal)):\n",
    "  cluster_num = y_normal[i]\n",
    "  min_distance = normal_distances[i][cluster_num]\n",
    "  center_distances[cluster_num].append(min_distance)\n",
    "\n",
    "# print(center_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHTx1AlaSEAV"
   },
   "source": [
    "**Hint:** Use `kmeans.transform(normal_data)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtjZEfaR2EI0"
   },
   "source": [
    "Now let's find the 99 percentile boundary for these clusters as we did in the previous example. This will help us define the boundary to differentiate between normal and abnormal data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rY-jotWuy43M"
   },
   "outputs": [],
   "source": [
    "percentile_treshold = 99\n",
    "\n",
    "\n",
    "# out of 1437 data samples, it finds the 2nd max (or max among 1436 samples) distance.\n",
    "center_99percentile_distance = dict()\n",
    "for key in center_distances.keys():\n",
    "    center_99percentile_distance[key] = np.percentile(center_distances[key], percentile_threshold)\n",
    "\n",
    "    # print(center_distances[key])\n",
    "    # print(np.max(center_distances[key]))\n",
    "    # print()\n",
    "\n",
    "\n",
    "print(center_99percentile_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2ltV4eyR8LV"
   },
   "source": [
    "### Classifying Test Data\n",
    "Now lets get the distance to each centroid for our anomalous data and combine it with our normal data. Then we can classify everything as either normal or abnormal based on the distances we calculated previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HvOnQNvJSnfu"
   },
   "source": [
    "We will first get our predictions for the anomalous data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PAMKsIpGSyUr"
   },
   "outputs": [],
   "source": [
    "y_abnormal = kmeans.predict(abnormal_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZvs5VqES3ZB"
   },
   "source": [
    "Then we combine the training data samples and testing data samples to visualize all samples on one plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pj7x4NjjR8LV"
   },
   "outputs": [],
   "source": [
    "# distances of each of the 360 testing samples from each of the 8 centroids\n",
    "abnormal_distances = kmeans.transform(abnormal_data)\n",
    "\n",
    "\n",
    "#combine all the data\n",
    "combined_distances = [*normal_distances, *abnormal_distances]\n",
    "combined_y = [*y_normal, *y_abnormal]\n",
    "normal_data_length = len(normal_data)\n",
    "all_data = np.array([*normal_data, *abnormal_data])\n",
    "\n",
    "\n",
    "\n",
    "false_neg=0\n",
    "false_pos=0\n",
    "\n",
    "colors = []\n",
    "for i in range(len(all_data)):\n",
    "  min_distance = combined_distances[i][combined_y[i]]\n",
    "  cluster_num = combined_y[i]\n",
    "\n",
    "  if (min_distance > center_99percentile_distance[cluster_num]):   # anomalous - positive\n",
    "    if (i<normal_data_length):       # training data is the first 1437 elements in the combined list\n",
    "      false_pos+=1\n",
    "\n",
    "  else:               # normal - negative\n",
    "    if (i>=normal_data_length):      # anomalous data is the last 360 elements in the combined list\n",
    "      false_neg+=1\n",
    "\n",
    "\n",
    "\n",
    "print('Normal datapoints misclassified as abnormal: ', false_pos)             # abnormal assumed as positive\n",
    "print('Abnormal datapoints misclassified as normal: ', false_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhkPZ6ZXW3ri"
   },
   "source": [
    "---\n",
    "\n",
    "\n",
    "**Question 4**: Why aren't we visualizing the clusters here?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STL0sK-GR8LV"
   },
   "source": [
    "The results are ok but not ideal. We can try adjusting the `percentile_treshold` variable to get better results. However, generally, K-means doesn't scale well with increased dimensionality. This example is still very low dimensional compared to many real world use cases and it still has a significant impact on our accuracy. This relationship is called the [***Curse of Dimensionality***](https://en.wikipedia.org/wiki/Curse_of_dimensionality) which plagues many conventional machine learning algorithms.\n",
    "\n",
    "**Next we will explore how to improve our K-means results with some pre-processing.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3IkAByRJ_G9"
   },
   "source": [
    "# 3. Dimensionality Reduction\n",
    "\n",
    "To combat the ***Curse of Dimensionality*** we can try projecting our data into a low dimensional space.\n",
    "\n",
    "We can use the t-distributed stochastic neighbor embedding ([t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)) algorithm, to pre-process the data before feeding it into K-means. t-SNE is used to visualize high dimensional data and we can use it to reduce the dimensionality of our input data which will hopefully lead to better results!\n",
    "\n",
    "First we will run TSNE on all of our data (normal and abnormal) and later split it into our train (normal) and test (abnormal) data. This is becasue t-SNE is a transductive learner and is not intended to transform data beyond what it is trained on. There are some recent [implementations](https://github.com/kylemcdonald/Parametric-t-SNE) of a [parametric t-sne algorithm](https://lvdmaaten.github.io/publications/papers/AISTATS_2009.pdf) that can acomplish this but they are not included in sklearn.\n",
    "\n",
    "We project our data from 64 dimensions to 2 dimensions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "997kxAbzR8LW"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Project the data to two dimensions: this step will take several seconds\n",
    "\n",
    "# instantiating tSNE with 2-dimensional data\n",
    "tsne = TSNE(n_components=2, init='random', random_state=0)\n",
    "digits_proj = tsne.fit_transform(digits.data)        # transform on all classes (0 -9)\n",
    "print(digits_proj.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbFKo1UD6ZZg"
   },
   "source": [
    "### Visualization of data after Dimensionality Reduction\n",
    "We visualize our transformed data and observe that we have nicely separated two dimension data (down from our 64 pixel images)! This looks a lot more like the clusters in the first example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Fw6CB01LU02"
   },
   "outputs": [],
   "source": [
    "#Visualize our new data\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(digits_proj[:, 0], digits_proj[:, 1],c=digits.target, s=50, cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tg0wHyv-RFfH"
   },
   "source": [
    "\n",
    "\n",
    "Next, we separate the data into normal and abnormal just like the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OMovdjEIREZ5"
   },
   "outputs": [],
   "source": [
    "normal_data = []\n",
    "abnormal_data = []\n",
    "\n",
    "normal_label = []\n",
    "abnormal_label = []\n",
    "\n",
    "num_clusters = 8\n",
    "\n",
    "#separate our data arbitrarily into normal (2-9) and abnormal (0-1)\n",
    "for i in range(len(digits.target)):\n",
    "  if digits.target[i]<10-num_clusters:\n",
    "    abnormal_data.append(digits_proj[i])\n",
    "    abnormal_label.append(digits.target[i])\n",
    "  else:\n",
    "    normal_data.append(digits_proj[i])\n",
    "    normal_label.append(digits.target[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVDatE4yXmtB"
   },
   "source": [
    "Now run K-means and calculate our percentile threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBS7yNCnK2kw"
   },
   "outputs": [],
   "source": [
    "# Compute the clusters\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
    "kmeans.fit(normal_data)\n",
    "y_normal = kmeans.predict(normal_data)\n",
    "\n",
    "\n",
    "# distances of each of the 1437 normal training samples from each of the 8 centroids\n",
    "normal_distances = kmeans.transform(normal_data)\n",
    "print(normal_distances.shape)\n",
    "\n",
    "\n",
    "center_distances = {key: [] for key in range(num_clusters)}\n",
    "for i in range(len(y_normal)):\n",
    "  min_distance = normal_distances[i][y_normal[i]]\n",
    "  center_distances[y_normal[i]].append(min_distance)\n",
    "\n",
    "\n",
    "percentile_threshold = 99\n",
    "# out of 1437 data samples, it finds the 2nd max (or max among 1436 samples) distance.\n",
    "center_99percentile_distance = dict()\n",
    "for key in center_distances.keys():\n",
    "    center_99percentile_distance[key] = np.percentile(center_distances[key], percentile_threshold)\n",
    "\n",
    "    # print(center_distances[key])\n",
    "    # print(np.max(center_distances[key]))\n",
    "    # print()\n",
    "\n",
    "print()\n",
    "print(center_99percentile_distance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQkSkEUrX0U1"
   },
   "source": [
    "Finally we calculate the distances of our abnormal data to our cluster centers. Then count the number of false negatives and false positives and plot all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISboi3-sTaiL"
   },
   "outputs": [],
   "source": [
    "y_abnormal = kmeans.predict(abnormal_data)\n",
    "abnormal_distances = kmeans.transform(abnormal_data)\n",
    "\n",
    "#combine all the data\n",
    "combined_distances = [*normal_distances, *abnormal_distances]\n",
    "combined_y = [*y_normal, *y_abnormal]\n",
    "normal_data_length = len(normal_data)\n",
    "all_data = np.array([*normal_data, *abnormal_data])\n",
    "\n",
    "\n",
    "\n",
    "false_neg=0\n",
    "false_pos=0\n",
    "colors = []\n",
    "\n",
    "for i in range(len(all_data)):\n",
    "  min_distance = combined_distances[i][combined_y[i]]\n",
    "  cluster_num = combined_y[i]\n",
    "\n",
    "  if (min_distance > center_99percentile_distance[cluster_num]):   # anomalous - positive\n",
    "    colors.append(10)  # any 4th color\n",
    "    if (i<normal_data_length):       # training data is the first 1437 elements in the combined list\n",
    "      false_pos+=1\n",
    "\n",
    "  else:               # normal - negative\n",
    "    colors.append(cluster_num)\n",
    "    if (i>=normal_data_length):      # anomalous data is the last 360 elements in the combined list\n",
    "      false_neg+=1\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(all_data[:, 0], all_data[:, 1], c=colors, s=50, cmap='viridis')\n",
    "\n",
    "centers = kmeans.cluster_centers_\n",
    "for i in range(len(centers)):\n",
    "  circle = plt.Circle((centers[i][0], centers[i][1]),center_99percentile_distance[i], color='black', alpha=0.1);\n",
    "  ax.add_artist(circle)\n",
    "\n",
    "print('Normal datapoints misclassified as abnormal: ', false_pos)\n",
    "print('Abnormal datapoints misclassified as normal: ', false_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdGuqSYiYKv_"
   },
   "source": [
    "We've reduced the number of abnormal points being misclassified as normal!!\n",
    "\n",
    "Unfortunately, while dimensionality reduction can be a power tool, it's not always a viable option. Algorithms like t-SNE can take a long time to run and won't always produce useful results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCYghaoq6gwF"
   },
   "source": [
    "## Additional Readings\n",
    "If you would like to learn about using reconstruction with K-means for anomaly detection check out: http://amid.fish/anomaly-detection-with-k-means-clustering"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
